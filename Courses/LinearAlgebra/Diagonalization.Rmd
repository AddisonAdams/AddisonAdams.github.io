---
title: "Diagonalization"
author: "Brody Erlandson"
output:
  html_document:
    code_folding: hide
    toc: true
    toc_float: true
    toc_depth: 2
---

\renewcommand{\v}{\mathbf{v}}
\renewcommand{\u}{\mathbf{u}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\vx}{\mathbf{x}}
\newcommand{\vy}{\mathbf{y}}
\newcommand{\vz}{\mathbf{z}}
\newcommand{\0}{\mathbf{0}}
\newcommand{\1}{\mathbf{1}}
\newcommand{\A}{\mathbf{A}}
\newcommand{\B}{\mathbf{B}}
\newcommand{\Cmat}{\mathbf{C}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\set}[2]{\{#1_1, #1_2, \dots, #1_#2\}}
\newcommand{\null}[1]{\mathcal{N}(#1)}
\newcommand{\col}[1]{\mathcal{C}(#1)}
\newcommand{\row}[1]{\mathcal{R}(#1)}
\newcommand{\eig}{\mathcal{E}}
\newcommand{\vdd}[2]{\begin{bmatrix} #1\\ #2 \end{bmatrix}}
\newcommand{\vddd}[3]{\begin{bmatrix} #1\\ #2\\ #3 \end{bmatrix}}
\newcommand{\vdddd}[4]{\begin{bmatrix} #1\\ #2\\ #3\\ #4 \end{bmatrix}}
\newcommand{\mdd}[4]{\begin{bmatrix} #1 & #2\\ #3 & #4 \end{bmatrix}}
\newcommand{\mddd}[9]{\begin{bmatrix} #1 & #2 & #3\\ #4 & #5 & #6\\ #7 & #8 & #9\\\end{bmatrix}}
\newcommand{\span}[1]{\langle #1 \rangle}
\newcommand{\innerProd}[2]{\langle #1, #2 \rangle}
\newcommand{\norm}[1]{\lVert #1 \rVert}
\newcommand{\dim}[1]{\text{dim}(#1)}
\newcommand{\det}{\text{det}}
\newcommand{\kernel}{\mathcal{K}}
\newcommand{\range}{\mathcal{R}}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Similarity and Diagonalization

## Definitions and Theorems {.tabset}

### Definition 1.1. 
**Similar Matrices**

If $\A$ and $\B$ are $n\times n$ matrices, $\bf P$ is an invertible $n\times n$ matrix, and $\A = \mathbf{PBP}^{−1}$, then $\A$ and $\B$ are similar.

### Theorem 1.1.
**Similar Matrices Have Same Eigenvalues**

If $\A$ and $\B$ are similar, then they have the same characteristic polynomial and therefore the same eigenvalues.

<details><summary>*Proof*</summary> 

Let $\A = \mathbf{PBP}^{-1}$,

$$ |\A-\lambda\I| = |\mathbf{PBP}^{-1}-\lambda\mathbf{PP}^{-1}| = |\mathbf{P}(\mathbf{BP}^{-1}-\lambda\mathbf{P}^{-1})|
\\ = |\mathbf{P}(\mathbf{B}-\lambda\I)\mathbf{P}^{-1}| = |\mathbf{P}||\mathbf{B}-\lambda\I||\mathbf{P}^{-1}|= |\mathbf{P}||\mathbf{P}^{-1}||\mathbf{B}-\lambda\I|
\\ = |\mathbf{P}\mathbf{P}^{-1}||\mathbf{B}-\lambda\I| = |\I||\mathbf{B}-\lambda\I| = |\mathbf{B}-\lambda\I|$$

</details> \newline 

### Definition 1.2.
**Diagonalizable Matrix**

An $n \times n$ matrix $\A$ is diagonalizable if it is similar to a diagonal matrix $\bf D$. In other words, if $\A = \mathbf{PDP}^{−1}$, where $\bf P$ is an invertible $n \times n$ matrix.

### Theorem 1.2.
**The Diagonalization Theorem**

An $n\times n$ matrix $\A$ is diagonalizable iff there exists a set of $n$ linearly independent eigenvectors of $\A$. Furthermore, if $\A$ is diagonalizable as $\A = \mathbf{PDP}^{−1}$, then the columns of $\bf P$ are $n$ linearly independent eigenvectors of $\A$, and the diagonal entries of $\bf D$ are the corresponding eigenvalues of $\A$.

### Theorem 1.3. 
**Distinct Eigenvalues Means Diagonalizable**

An $n \times n$ matrix $\A$ is diagonalizable if it has $n$ distinct eigenvalues.

<details><summary>*Proof*</summary>

Since $\A$ has distinct eigenvalues, the set containing an eigenvector corresponding to each eigenvalue is linearly independent. By the Diagonalization
Theorem, $\A$ is therefore diagonalizable.

</details> \newline 

### Theorem 1.4.
**Diagonalizable Means Equal Multiplicities**

An $n \times n$ matrix $\A$ is diagonalizable iff $\gamma_A(\lambda) = \alpha_A(\gamma)$ for every eigenvalue $\lambda$ of $\A$.

### Theorem 1.5.
**Similarity and Change of Basis**

If $T: V \rightarrow V$ is a linear transformation and $B$ and $C$ are bases for $V$, then

$$
M_{B, B}^{T}=C_{B, C}^{-1} M_{C, C}^{T} C_{B, C}
$$

## Examples

Consider the matrix $A$ we saw earlier.

$$
A=\left[\begin{array}{ll}
1 & 2 \\
3 & 2
\end{array}\right]
$$

<details><summary>Solution</summary>

We found its eigenvalues to be $-1$ and 4 , with bases

$$
\left\{\left[\begin{array}{r}
-1 \\
1
\end{array}\right]\right\}, \quad \text { and }\left\{\left[\begin{array}{l}
2 \\
3
\end{array}\right]\right\}
$$

respectively. Since the eigenvalues are distinct, we know $A$ is diagonalizable, and can easily construct $P$ and $D$ :

$$
P=\left[\begin{array}{rr}
-1 & 2 \\
1 & 3
\end{array}\right] \quad D=\left[\begin{array}{rr}
-1 & 0 \\
0 & 4
\end{array}\right]
$$

Note, since $P$ is invertible we generally do not compute $P^{-1}$, since once we have $P$, it's a trivial matter to get $P^{-1}$. Furthermore, we can check $P$ and $D$ as follows:

$$
\begin{aligned}
A &=P D P^{-1} \\
A P &=P D P^{-1} P \\
A P &=P D
\end{aligned}
$$

So if $A P=P D$, then $A=P D P^{-1}$. In this case

$$
\begin{gathered}
A P=\left[\begin{array}{ll}
1 & 2 \\
3 & 2
\end{array}\right]\left[\begin{array}{rr}
-1 & 2 \\
1 & 3
\end{array}\right]=\left[\begin{array}{rc}
1 & 8 \\
-1 & 12
\end{array}\right] \\
P D=\left[\begin{array}{rr}
-1 & 2 \\
1 & 3
\end{array}\right]\left[\begin{array}{rr}
-1 & 0 \\
0 & 4
\end{array}\right]=\left[\begin{array}{rr}
1 & 8 \\
-1 & 12
\end{array}\right]
\end{gathered}
$$

</details>

Diagonalize $A$ if possible.

$$
A=\left[\begin{array}{lll}
1 & -3 & 3 \\
3 & -5 & 3 \\
6 & -6 & 4
\end{array}\right]
$$

<details><summary>Solutions</summary>

Previously we found the eigenvalues of $A$ to be 4 and $-2$, and $-2$ had an algebraic multiplicity of 2 . The corresponding eigenspaces had bases

$$
\left\{\left[\begin{array}{l}
1 \\
1 \\
2
\end{array}\right]\right\} \text { and }\left\{\left[\begin{array}{l}
1 \\
1 \\
0
\end{array}\right],\left[\begin{array}{r}
-1 \\
0 \\
1
\end{array}\right]\right\}
$$

respectively. By theorem $1.4$ above, we know that $A$ is diagonalizable since

$$
\begin{gathered}
\gamma_{A}(4)=1=\alpha_{A}(4) \\
\gamma_{A}(-2)=2=\alpha_{A}(-2)
\end{gathered}
$$

So we can construct $P$ and $D$ as usual:

$$
P=\left[\begin{array}{rrr}
1 & 1 & -1 \\
1 & 1 & 0 \\
2 & 0 & 1
\end{array}\right] \quad D=\left[\begin{array}{rrr}
4 & 0 & 0 \\
0 & -2 & 0 \\
0 & 0 & -2
\end{array}\right]
$$

</details>

 Find a basis $D$ for $\mathbb{P}_{2}$ that yields a diagonal matrix representation of the linear transformation $T: \mathbb{P}_{2} \rightarrow \mathbb{P}_{2}$ given by

$$
T\left(a+b x+c x^{2}\right)=(a-3 b+3 c)+(3 a-5 b+3 c) x+(6 a-6 b+4 c) x^{2}
$$

<details><summary>Solutions</summary>

Here are the steps we'll do:

1. Choose an "easy" basis $B$ for $\mathbb{P}_{2}$

2. Find $M_{B, B}^{T}$

3. Find the eigenvalues of $M_{B, B}^{T}$, which are the same as the eigenvalues of $T$

4. Find bases for the eigenspaces of $M_{B, B}^{T}$

5. Use the inverse transformation $\rho_{B}^{-1}$ to find the eigenvectors of $T$

6. Construct a basis $D$ from the eigenvectors of $T$

7. Check to see that $M_{D, D}^{T}$ is indeed a diagonal matrix

Here we go:

- **Step 1**

As usual, choose the standard basis for $\mathbb{P}_{2}$ :

$$
B=\left\{1, x, x^{2}\right\}
$$

- **Step 2**

First find the images of the basis vectors in $B$ under $T$ :

$$
\begin{aligned}
T(a+b x&\left.+c x^{2}\right)=(a-3 b+3 c)+(3 a-5 b+3 c) x+(6 a-6 b+4 c) x^{2} \\
T(1) &=T\left(1+0 x+0 x^{2}\right) \\
&=(1-3(0)+3(0))+(3(1)-5(0)+3(0)) x+(6(1)-6(0)+4(0)) x^{2} \\
&=1+3 x+6 x^{2} \\
T(x) &=T\left(0+1 x+0 x^{2}\right)=-3-5 x-6 x^{2} \\
T\left(x^{2}\right) &=T\left(0+0 x+1 x^{2}\right)=3+3 x+4 x^{2}
\end{aligned}
$$

By observation we have

$$
\rho_{B}(T(1))=\left[\begin{array}{l}
1 \\
3 \\
6
\end{array}\right] \quad \rho_{B}(T(x))=\left[\begin{array}{c}
-3 \\
-5 \\
-6
\end{array}\right] \quad \rho_{B}\left(T\left(x^{2}\right)\right)=\left[\begin{array}{l}
3 \\
3 \\
4
\end{array}\right]
$$

and

$$
M_{B, B}^{T}=\left[\begin{array}{ccc}
1 & -3 & 3 \\
3 & -5 & 3 \\
6 & -6 & 4
\end{array}\right]
$$

- **Steps 3 and 4**

In the notes on Eigenvalues and Eigenvectors, $M_{B, B}^{T}$ was one of the matrices in the examples. We found the eigenvalues and corresponding eigenspaces to be:

$$
\begin{gathered}
\lambda_{1}=4, \quad \mathcal{E}_{M}(4)=\left\langle\left\{\left[\begin{array}{l}
1 \\
1 \\
2
\end{array}\right]\right\}\right\rangle \\
\lambda_{2}=-2, \quad \mathcal{E}_{M}(-2)=\left\langle\left\{\left[\begin{array}{l}
1 \\
1 \\
0
\end{array}\right],\left[\begin{array}{r}
-1 \\
0 \\
1
\end{array}\right]\right\}\right\rangle
\end{gathered}
$$

- **Step 5**

Now to find the eigenvectors of $T$, we use the inverse transformation $\rho_{B}^{-1}$ on the eigenvectors of $M_{B, B}^{T}$ :

$$
\begin{aligned}
&\rho_{B}^{-1}\left(\left[\begin{array}{l}
1 \\
1 \\
2
\end{array}\right]\right)=1 \cdot 1+1 x+2 x^{2}=1+x+2 x^{2} \\
&\rho_{B}^{-1}\left(\left[\begin{array}{l}
1 \\
1 \\
0
\end{array}\right]\right)=1 \cdot 1+1 x+0 x^{2}=1+x \\
&\rho_{B}^{-1}\left(\left[\begin{array}{r}
-1 \\
0 \\
1
\end{array}\right]\right)=-1 \cdot 1+0 x+1 x^{2}=-1+x^{2}
\end{aligned}
$$

- **Step 6**

This gives the eigenvector basis for $\mathbb{P}_{2}$.

$$
D=\left\{1+x+2 x^{2}, 1+x,-1+x^{2}\right\}
$$

- **Step 7**

First find the images of the basis vectors under $T$ :

$$
\begin{aligned}
T\left(a+b x+c x^{2}\right) &=(a-3 b+3 c)+(3 a-5 b+3 c) x+(6 a-6 b+4 c) x^{2} \\
T\left(1+x+2 x^{2}\right) &=(1-3(1)+3(2))+(3(1)-5(1)+3(2)) x+(6(1)-6(1)+4(2)) x^{2} \\
&=4+4 x+8 x^{2} \\
T(1+x) &=(1-3(1)+3(0))+(3(1)-5(1)+3(0)) x+(6(1)-6(1)+4(0)) x^{2} \\
&=-2-2 x \\
T\left(-1+x^{2}\right) &=(-1-3(0)+3(1))+(3(-1)-5(0)+3(1)) x+(6(-1)-6(0)+4(1)) x^{2} \\
&=2-2 x^{2}
\end{aligned}
$$

Next we want the coordinate vectors of these polynomials relative to $D$.

$T\left(1+x+2 x^{2}\right)=4+4 x+8 x^{2}$

$$
4+4 x+8 x^{2}=c_{1}\left(1+x+2 x^{2}\right)+c_{2}(1+x)+c_{3}\left(-1+x^{2}\right)
$$

By observation:

$$
\rho_{D}\left(4+4 x+8 x^{2}\right)=\left[\begin{array}{l}
c_{1} \\
c_{2} \\
c_{3}
\end{array}\right]=\left[\begin{array}{l}
4 \\
0 \\
0
\end{array}\right]
$$

$T(1+x)=-2-2 x$

$$
\begin{gathered}
-2-2 x=c_{1}\left(1+x+2 x^{2}\right)+c_{2}(1+x)+c_{3}\left(-1+x^{2}\right) \\
\rho_{D}(-2-2 x)=\left[\begin{array}{l}
c_{1} \\
c_{2} \\
c_{3}
\end{array}\right]=\left[\begin{array}{r}
0 \\
-2 \\
0
\end{array}\right] \\
=2-2 x^{2} \\
2-2 x^{2}=c_{1}\left(1+x+2 x^{2}\right)+c_{2}(1+x)+c_{3}\left(-1+x^{2}\right) \\
\rho_{D}(-2-2 x)=\left[\begin{array}{l}
c_{1} \\
c_{2} \\
c_{3}
\end{array}\right]=\left[\begin{array}{r}
0 \\
0 \\
-2
\end{array}\right]
\end{gathered}
$$

$$
\begin{aligned}
& T\left(-1+x^{2}\right)=2-2 x^{2}
\end{aligned}
$$

and

$$
M_{D, D}^{T}=\left[\begin{array}{rrr}
4 & 0 & 0 \\
0 & -2 & 0 \\
0 & 0 & -2
\end{array}\right]
$$

which is a diagonal matrix, with the eigenvalues of $T$ on the diagonal, as expected.

</details>

# Orthonormal Diagonalization

## Definitions and Theorems {.tabset}

### Definition 2.1.
**Normal Matrix**

An $n \times n$ matrix $A$ is normal if

$$
A A^{*}=A^{*} A
$$

### Theorem 2.1.
**Orthonormal Diagonalization**

If $A$ is an $n \times n$ matrix then there is a unitary matrix $U$ and a diagonal matrix $D$, with diagonal entries equal to the eigenvalues of $A$, such that

$$
A=U D U^{*}
$$

iff $A$ is a normal matrix. Moreover, the columns of $U$ will be a set of orthonormal eigenvectors of $A$ that form a basis for $\mathbb{C}^{n}$.

### Definition 2.2.
**Symmetric Matrix**

An $n \times n$ matrix $A$ with real entries is symmetric if

$$
A=A^{T}
$$

Recall that a hermitian matrix $A$ satisfies the the property

$$
A=A^{*}
$$

Symmetric matrices are therefore a special case of hermitian matrices. If $A$ is a real matrix (i.e. a matrix with all real entries), then

$$
A=\bar{A}
$$

$$
A^{*}=(\bar{A})^{T}=A^{T}
$$

Another way to say this is that a real hermitian matrix is symmetric. Theorem 2.2. Symmetric Matrices Have Real Eigenvalues

If $A$ is a symmetric, then it has real eigenvalues and eigenvectors.

<details><summary>*Proof*</summary>

Since $A$ is symmetric, it is hermitian and therefore its eigenvalues are real. Let $\lambda$ be an eigenvalue of $A$ and consider

$$
(A-\lambda I) \mathbf{x}=\mathbf{0}
$$

Since $A$ and $\lambda$ are real, $(A-\lambda I)$ is also real. When row-reducing $(A-\lambda I)$ to solve this equation, the entries in the matrix will remain real and therefore the solution vector $\mathbf{x}$ will be in $\mathbb{R}^{n}$. Since $\mathbf{x}$ is an eigenvector corresponding to the eigenvalue $\lambda$, the eigenvectors of $A$ are also real.

</details>

### Definition 2.3.
**Orthogonal Matrix**

An $n \times n$ matrix $A$ with real entries is orthogonal if

$$
A A^{T}=A^{T} A=I_{n}
$$

Recall that a unitary matrix satisfies the property

$$
U^{*} U=U U^{*}=I
$$

Therefore an orthogonal matrix is simply a real unitary matrix.

### Definition 2.4. 
**Orthogonally Diagonalizable Matrix**

An $n \times n$ real matrix $A$ is orthogonally diagonalizable if there exists an orthogonal matrix $P$ and a real diagonal matrix $D$ such that

$$
A=P D P^{T}=P D P^{-1}
$$

### Theorem 2.3. 
**Symmetric Matrices Are Orthogonally Diagonalizable**

An $n \times n$ matrix $A$ is orthogonally diagonalizable iff $A$ is symmetric.

<details><summary>*Proof*</summary>

If $A$ is orthogonally diagonalizable as $A=P D P^{T}$, then

$$
A^{T}=\left(P D P^{T}\right)^{T}=\left(P^{T}\right)^{T} D^{T} P^{T}=P D P^{T}=A
$$

therefore $A$ is symmetric.

Conversely, if $A$ is symmetric, then it is normal, and by theorem $2.1$ it is diagonalizable as

$$
A=U D U^{*}
$$

where $U$ is unitary with columns that are eigenvectors of $A$, and $D$ is diagonal with eigenvalues of $A$ on the main diagonal. Since $A$ is symmetric, its eigenvalues and eigenvectors are real, therefore $U$ is an orthogonal matrix and $D$ is real, and $A$ is orthogonally diagonalizable.

</details>

### Theorem 2.4. 

The *Spectral Theorem for Real Matrices* an $n \times n$ symmetric matrix A has has the following properties:

1. $\A$ has $n$ real eigenvalues (counting multiplicites).

2. If $\lambda$ is an eigenvalue of $A$, then $\gamma_{A}(\lambda)=\alpha_{A}(\lambda)$.

3. The eigenspaces of $\A$ are mutually orthogonal (eigenvectors corresponding to distinct eigenvalues are orthogonal).

4. $\A$ is orthogonally diagnonalizable. 

## Examples

Let

$$
A=\left[\begin{array}{rrr}
7 & -4 & 4 \\
-4 & 5 & 0 \\
4 & 0 & 9
\end{array}\right]
$$

Orthogonally diagonalize $A$ if possible.

<details><summary>Solution</summary>

The eigenvalues and associated eigenspace basis vectors for $A$ are:

$$
\begin{aligned}
&\lambda_{1}=13, \quad \mathbf{v}_{1}=\left[\begin{array}{r}
2 \\
-1 \\
2
\end{array}\right] \\
&\lambda_{2}=7, \quad \mathbf{v}_{2}=\left[\begin{array}{r}
-1 \\
2 \\
2
\end{array}\right] \\
&\lambda_{3}=1, \quad \mathbf{v}_{3}=\left[\begin{array}{r}
-2 \\
-2 \\
1
\end{array}\right]
\end{aligned}
$$

To construct the orthogonal matrix $P$ we need to normalize the basis vectors:

$$
\begin{aligned}
&\left\|\mathbf{v}_{1}\right\|=\sqrt{2^{2}+(-1)^{2}+2^{2}}=\sqrt{9}=3, \quad \mathbf{u}_{1}=\frac{\mathbf{v}_{1}}{\left\|\mathbf{v}_{1}\right\|}=\left[\begin{array}{r}
2 / 3 \\
-1 / 3 \\
2 / 3
\end{array}\right] \\
&\left\|\mathbf{v}_{2}\right\|=\sqrt{(-1)^{2}+2^{2}+2^{2}}=\sqrt{9}=3, \quad \mathbf{u}_{2}=\frac{\mathbf{v}_{2}}{\left\|\mathbf{v}_{2}\right\|}=\left[\begin{array}{r}
-1 / 3 \\
2 / 3 \\
2 / 3
\end{array}\right] \\
&\left\|\mathbf{v}_{3}\right\|=\sqrt{(-2)^{2}+(-2)^{2}+1^{2}}=\sqrt{9}=3, \quad \mathbf{u}_{3}=\frac{\mathbf{v}_{3}}{\left\|\mathbf{v}_{3}\right\|}=\left[\begin{array}{r}
-2 / 3 \\
-2 / 3 \\
1 / 3
\end{array}\right]
\end{aligned}
$$

So

$$
P=\frac{1}{3}\left[\begin{array}{rrr}
2 & -1 & -2 \\
-1 & 2 & -2 \\
2 & 2 & 1
\end{array}\right], \quad D=\left[\begin{array}{ccc}
13 & 0 & 0 \\
0 & 7 & 0 \\
0 & 0 & 1
\end{array}\right]
$$

And

$$
A=P D P^{T}
$$

</details>

Let

$$
A=\left[\begin{array}{llll}
4 & 0 & 1 & 0 \\
0 & 4 & 0 & 1 \\
1 & 0 & 4 & 0 \\
0 & 1 & 0 & 4
\end{array}\right]
$$

Find an orthonormal basis $B$ for $\mathbb{R}^{4}$ consisting of eigenvectors of $A$, and orthogonally diagonalize $A$.

<details><summary>Solution</summary>

The eigenvalues, eigenspaces, and multiplicites for $A$ are:

$$
\begin{aligned}
&\lambda_{1}=5, \quad \mathcal{E}_{A}(5)=\left\langle\left\{\left[\begin{array}{l}
0 \\
1 \\
0 \\
1
\end{array}\right],\left[\begin{array}{l}
1 \\
0 \\
1 \\
0
\end{array}\right]\right\}\right\rangle, \alpha_{A}(5)=2, \gamma_{A}(5)=2 \\
&\lambda_{2}=3, \quad \mathcal{E}_{A}(3)=\left\langle\left\{\left[\begin{array}{r}
0 \\
-1 \\
0 \\
1
\end{array}\right],\left[\begin{array}{r}
-1 \\
0 \\
1 \\
0
\end{array}\right]\right\}\right\rangle, \alpha_{A}(3)=2, \gamma_{A}(3)=2
\end{aligned}
$$

To construct the orthogonal matrix $P$ we need to normalize the basis vectors. Let

$$
\mathbf{v}_{1}=\left[\begin{array}{l}
0 \\
1 \\
0 \\
1
\end{array}\right], \quad \mathbf{v}_{2}=\left[\begin{array}{l}
1 \\
0 \\
1 \\
0
\end{array}\right], \quad \mathbf{v}_{3}=\left[\begin{array}{r}
0 \\
-1 \\
0 \\
1
\end{array}\right], \quad \mathbf{v}_{4}=\left[\begin{array}{r}
-1 \\
0 \\
1 \\
0
\end{array}\right]
$$

Then, conveniently we have

$$
\left\|\mathbf{v}_{1}\right\|=\left\|\mathbf{v}_{2}\right\|=\left\|\mathbf{v}_{3}\right\|=\left\|\mathbf{v}_{4}\right\|=\sqrt{2}
$$

$\mathbf{u}_{1}=\frac{\mathbf{v}_{1}}{\left\|\mathbf{v}_{1}\right\|}=\left[\begin{array}{c}0 \\ \frac{1}{\sqrt{2}} \\ 0 \\ \frac{1}{\sqrt{2}}\end{array}\right], \quad \mathbf{u}_{2}=\frac{\mathbf{v}_{2}}{\left\|\mathbf{v}_{2}\right\|}=\left[\begin{array}{c}\frac{1}{\sqrt{2}} \\ 0 \\ \frac{1}{\sqrt{2}} \\ 0\end{array}\right], \quad \mathbf{u}_{3}=\frac{\mathbf{v}_{3}}{\left\|\mathbf{v}_{3}\right\|}=\left[\begin{array}{c}0 \\ -\frac{1}{\sqrt{2}} \\ 0 \\ \frac{1}{\sqrt{2}}\end{array}\right], \quad \mathbf{u}_{4}=\frac{\mathbf{v}_{4}}{\left\|\mathbf{v}_{4}\right\|}=\left[\begin{array}{c}-\frac{1}{\sqrt{2}} \\ 0 \\ \frac{1}{\sqrt{2}} \\ 0\end{array}\right]$ The orthonormal basis for $\mathbb{R}^{4}$ is

$$
B=\left\{\left[\begin{array}{c}
0 \\
\frac{1}{\sqrt{2}} \\
0 \\
\frac{1}{\sqrt{2}}
\end{array}\right],\left[\begin{array}{c}
\frac{1}{\sqrt{2}} \\
0 \\
\frac{1}{\sqrt{2}} \\
0
\end{array}\right],\left[\begin{array}{c}
0 \\
-\frac{1}{\sqrt{2}} \\
0 \\
\frac{1}{\sqrt{2}}
\end{array}\right],\left[\begin{array}{c}
-\frac{1}{\sqrt{2}} \\
0 \\
\frac{1}{\sqrt{2}} \\
0
\end{array}\right]\right\}
$$

To orthogonally diagonalize $A$ we have

$$
P=\left[\begin{array}{cccc}
0 & \frac{1}{\sqrt{2}} & 0 & -\frac{1}{\sqrt{2}} \\
\frac{1}{\sqrt{2}} & 0 & -\frac{1}{\sqrt{2}} & 0 \\
0 & \frac{1}{\sqrt{2}} & 0 & \frac{1}{\sqrt{2}} \\
\frac{1}{\sqrt{2}} & 0 & \frac{1}{\sqrt{2}} & 0
\end{array}\right], \quad D=\left[\begin{array}{cccc}
5 & 0 & 0 & 0 \\
0 & 5 & 0 & 0 \\
0 & 0 & 3 & 0 \\
0 & 0 & 0 & 3
\end{array}\right]
$$

Where

$$
A=P D P^{T}
$$

</details>











