---
title: "Eigenvalues and Eigenvectors"
author: "Brody Erlandson"
output:
  html_document:
    code_folding: hide
    toc: true
    toc_float: true
    toc_depth: 2
---

\renewcommand{\v}{\mathbf{v}}
\renewcommand{\u}{\mathbf{u}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\vx}{\mathbf{x}}
\newcommand{\vy}{\mathbf{y}}
\newcommand{\vz}{\mathbf{z}}
\newcommand{\0}{\mathbf{0}}
\newcommand{\1}{\mathbf{1}}
\newcommand{\A}{\mathbf{A}}
\newcommand{\B}{\mathbf{B}}
\newcommand{\Cmat}{\mathbf{C}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\set}[2]{\{#1_1, #1_2, \dots, #1_#2\}}
\newcommand{\null}[1]{\mathcal{N}(#1)}
\newcommand{\col}[1]{\mathcal{C}(#1)}
\newcommand{\row}[1]{\mathcal{R}(#1)}
\newcommand{\eig}{\mathcal{E}}
\newcommand{\vdd}[2]{\begin{bmatrix} #1\\ #2 \end{bmatrix}}
\newcommand{\vddd}[3]{\begin{bmatrix} #1\\ #2\\ #3 \end{bmatrix}}
\newcommand{\vdddd}[4]{\begin{bmatrix} #1\\ #2\\ #3\\ #4 \end{bmatrix}}
\newcommand{\mdd}[4]{\begin{bmatrix} #1 & #2\\ #3 & #4 \end{bmatrix}}
\newcommand{\mddd}[9]{\begin{bmatrix} #1 & #2 & #3\\ #4 & #5 & #6\\ #7 & #8 & #9\\\end{bmatrix}}
\newcommand{\span}[1]{\langle #1 \rangle}
\newcommand{\innerProd}[2]{\langle #1, #2 \rangle}
\newcommand{\norm}[1]{\lVert #1 \rVert}
\newcommand{\dim}[1]{\text{dim}(#1)}
\newcommand{\det}{\text{det}}
\newcommand{\kernel}{\mathcal{K}}
\newcommand{\range}{\mathcal{R}}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Direction Invariance and The Characteristic Equation

If $\A$ is an $n\times n$ matrix, $\vx$ is a vector in $\C^n$, and $\lambda$ is a complex number, consider the matrix equation: 

$$\A\vx = \lambda\vx$$

The geometric interpretation of this equation is that multiplication by $\A$ does not change the direction of $\vx$, only its length, which is scaled by $\lambda$.

## Definitions and Theorems {.tabset}

### Definition 1.1.
**Eigenvalues and Eigenvectors**

If $\A$ is an $n \times n$ matrix and there exists a non-trivial solution of the equation $\A\vx = \lambda\vx$, then the scalar $\lambda$ is an *eigenvalue* of $\A$, and $\vx$ is an *eigenvector* associated with $\lambda$.

*Note*: An eigenvector can never be zero, by definition. An eigenvalue however, can be zero.

*Note*: Only square matrices have eigenvalues and eigenvectors.

<details><summary>*How to find Eigenvalues and Eigenvectors*</summary> 

To find the eigenvalues and eigenvectors of a square matrix, we solve the equation $\A\vx = \lambda\vx$ for $\vx$:

$$ \A\vx = \lambda\vx \\
\A\vx − \lambda\vx = \0 \\
\A\vx − \lambda\I\vx = \0 \\
(\A − \lambda\I)\vx = \0$$

$\A−\lambda\I$ is an $n\times n$ matrix, so this equation has a non-trivial solution if and only if $\A − \lambda\I$ is singular (i.e. not invertible). By IMT, a matrix is not invertible iff its determinant is zero. So the eigenvalues of $\A$ are those values of $\lambda$ that make the determinant of $\A − \lambda\I$ equal to $0$.

</details> \newline 

### Definition 2.1.
**Characteristic Equation/Polynomial**

Given an $n \times n$ matrix $\A$, the characteristic polynomial of $\A$, $p_A(x)$, is

$$p_A(x) = |\A − xI|$$

and the characteristic equation of $\A$ is:

$$p_A(x) = 0$$

We solve the characteristic equation to find the eigenvalues of $\A$.

### Definition 3.1.
**Eigenspace**

Let $\A$ be a square matrix with eigenvalue $\lambda$. The set of eigenvectors associated with $\lambda$, together with the zero vector, is $\eig_A(\lambda)$, the eigenspace of $\A$ for $\lambda$. In other words,

$$ \eig_A(\lambda) = \null{\A − \lambda\I} $$

### Definition 3.2.
**Algebraic Multiplicity of an Eigenvalue**

Let $\A$ be a square matrix with eigenvalue $\lambda$. The algebraic mulitplicity of $\lambda$, $\alpha_A(\lambda)$, is the multiplicity of the zero $\lambda$ in $p_A(x)$. In other words, in the fully factored form of $p_A(x)$, $\alpha_A(\lambda)$ is the exponent of the $(x − \lambda)$ factor

### Definition 3.3.
**Geometric Multiplicity of an Eigenvalue**

Let $\A$ be a square matrix with eigenvalue $\lambda$. The geometric mulitplicity of $\lambda$, $\gamma_A(\lambda)$, is the dimension of the eigenspace $\eig_A(\lambda)$. $\gamma_A(\lambda) = \dim{\eig_A(\lambda)}$

## Examples

Find the eigenvalues and eigenvectors of $A$.

$$
A=\left[\begin{array}{ll}
1 & 2 \\
3 & 2
\end{array}\right]
$$

<details><summary>Solution</summary>

We solve the characteristic equation to find the eigenvalues:

$$
\begin{array}{r}
|A-x I|=0 \\
\left|\begin{array}{cc}
1-x & 2 \\
3 & 2-x
\end{array}\right|=0 \\
(1-x)(2-x)-6=0 \\
x^{2}-3 x-4=0 \\
(x+1)(x-4)=0 \\
x=-1 \text { or } x=4
\end{array}
$$

So the eigenvalues of $A$ are $-1$ and 4 .

To find the eigenvectors of $A$ associated with each eigenvalue we use the fact that $A \mathbf{x}=\lambda \mathbf{x}$ For $\lambda=-1$ :

$$
\begin{array}{r}
A \mathbf{x}=\lambda \mathbf{x} \\
A \mathbf{x}-\lambda \mathbf{x}=\mathbf{0} \\
A \mathbf{x}-\lambda I \mathbf{x}=\mathbf{0} \\
(A-\lambda I) \mathbf{x}=\mathbf{0} \\
(A-(-1) I) \mathbf{x}=\mathbf{0} \\
(A+I) \mathbf{x}=\mathbf{0} \\
{\left[\begin{array}{rr}
1+1 & 2 \\
3 & 2+1
\end{array}\right] \mathbf{x}=\mathbf{0}} \\
{\left[\begin{array}{ll}
2 & 2 \\
3 & 3
\end{array}\right] \mathbf{x}=\mathbf{0}}
\end{array}
$$

We solve this homogeneous equation as usual by row-reducing the coefficient matrix:

$$
\begin{gathered}
{\left[\begin{array}{ll}
2 & 2 \\
3 & 3
\end{array}\right] \sim\left[\begin{array}{ll}
1 & 1 \\
0 & 0
\end{array}\right] \Longrightarrow x_{1}+x_{2}=0, \quad x_{1}=-x_{2}} \\
\mathbf{x}=\left[\begin{array}{r}
-x_{2} \\
x_{2}
\end{array}\right]=x_{2}\left[\begin{array}{r}
-1 \\
1
\end{array}\right] \\
\mathbf{x}=t \mathbf{v}, \text { where } \mathbf{v}=\left[\begin{array}{r}
-1 \\
1
\end{array}\right], t \in \mathbb{C}, t \neq 0
\end{gathered}
$$

Note that the solution set is almost

$$
\left\langle\left\{\left[\begin{array}{r}
-1 \\
1
\end{array}\right]\right\}\right\rangle
$$

except that it doesn't contain the zero vector (since zero cannot be an eigenvector by definition, which is why $t \neq 0$ in the solution). The set of all the eigenvectors associated with the eigenvalue $-1$, together with the zero vetor (which is not an eigenvector) is a subspace of $\mathbb{C}^{2}$, since it is the span of a subset of $\mathbb{C}^{2}$. This subspace is called the eigenspace associated with the eigenvalue $-1$

For $\lambda=4$ :

So the eigenspace associated with the eigenvalue 4 is

$$
\mathcal{E}_{A}(4)=\left\langle\left\{\left[\begin{array}{l}
2 \\
3
\end{array}\right]\right\}\right\rangle
$$

$$
\begin{aligned}
& (A-\lambda I) \mathbf{x}=\mathbf{0} \\
& (A-4 I) \mathbf{x}=\mathbf{0} \\
& \left[\begin{array}{cc}1-4 & 2 \\3 & 2-4\end{array}\right] \mathbf{x}=\mathbf{0} \\
& \left[\begin{array}{rr}-3 & 2 \\3 & -2\end{array}\right] \mathbf{x}=\mathbf{0} \\
& \left[\begin{array}{rr}-3 & 2 \\3 & -2\end{array}\right] \sim\left[\begin{array}{rr}1 & -\frac{2}{3} \\0 & 0\end{array}\right] \Longrightarrow x_{1}-\frac{2}{3} x_{2}=0, \quad x_{1}=\frac{2}{3} x_{2} \\
& \mathbf{x}=\left[\begin{array}{c}\frac{2}{3} x_{2} \\x_{2}\end{array}\right]=x_{2}\left[\begin{array}{c}\frac{2}{3} \\1\end{array}\right] \\
& \mathbf{x}=t \mathbf{v}, \text { where } \mathbf{v}=\left[\begin{array}{l}2 \\3\end{array}\right], t \in \mathbb{C}, t \neq 0 
\end{aligned}
$$

</details>

Find the eigenvalues and eigenvectors of $A$.

$$
A=\left[\begin{array}{ccc}
1 & -3 & 3 \\
3 & -5 & 3 \\
6 & -6 & 4
\end{array}\right]
$$

<details><summary>Solution</summary>

Solve the characteristic equation to find the eigenvalues of $A$ :

$$|\bf A-x I_3| =0$$
$$\left|\begin{array}{ccc}
1-x & -3 & 3 \\
3 & -5-x & 3 \\
6 & -6 & 4-x
\end{array}\right|=0 \\
x^{3}-12 x-16=0 \\
(x-4)(x+2)(x+2)=0 \\
x=4 \text { or } x=-2
$$

So the eigenvalues of $A$ are $4,-2$.

Now we find the eigenspaces associated with each eigenvalue.

For $\lambda=4$ :

$$
\begin{gathered}
(A-\lambda I) \mathbf{x}=\mathbf{0} \\
{\left[\begin{array}{ccc}
1-4 & -3 & 3 \\
3 & -5-4 & 3 \\
6 & -6 & 4-4
\end{array}\right] \mathbf{x}=\mathbf{0}} \\
{\left[\begin{array}{ccc}
-3 & -3 & 3 \\
3 & -9 & 3 \\
6 & -6 & 0
\end{array}\right] \sim\left[\begin{array}{lll}
1 & 0 & -\frac{1}{2} \\
0 & 1 & -\frac{1}{2} \\
0 & 0 & 0
\end{array}\right]}
\end{gathered}
$$



$$
\begin{aligned}
\Longrightarrow x_{1}-\frac{1}{2} x_{3} &=0 \\
x_{2}-\frac{1}{2} x_{3} &=0 \\
x_{1} &=\frac{1}{2} x_{3} \\
x_{2} &=\frac{1}{2} x_{3}
\end{aligned}
$$

$$
\begin{aligned}
&\mathbf{x}=\left[\begin{array}{c}
\frac{1}{2} x_{3} \\
\frac{1}{2} x_{3} \\
x_{3}
\end{array}\right]=x_{3}\left[\begin{array}{c}
\frac{1}{2} \\
\frac{1}{2} \\
1
\end{array}\right] \\
&\mathbf{x}=t \mathbf{v}, \text { where } \mathbf{v}=\left[\begin{array}{l}
1 \\
1 \\
2
\end{array}\right], t \in \mathbb{C}, t \neq 0
\end{aligned}
$$

So the eigenspace associated with the eigenvalue 4 is

$$
\mathcal{E}_{A}(4)=\left\langle\left\{\left[\begin{array}{l}
1 \\
1 \\
2
\end{array}\right]\right\}\right\rangle
$$

For $\lambda=-2$ :

$$
\begin{aligned}
(A+2 I)=\left[\begin{array}{lll}
3 & -3 & 3 \\
3 & -6 & 3 \\
6 & -6 & 6
\end{array}\right] \sim\left[\begin{array}{rrr}
1 & -1 & 1 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{array}\right] \\
\Longrightarrow x_{1}-x_{2}+x_{3} &=0 \\
x_{1} &=x_{2}-x_{3}
\end{aligned}
$$



$$
\begin{aligned}
&\mathbf{x}=\left[\begin{array}{c}
x_{2}-x_{3} \\
x_{2} \\
x_{3}
\end{array}\right]=x_{2}\left[\begin{array}{l}
1 \\
1 \\
0
\end{array}\right]+x_{3}\left[\begin{array}{r}
-1 \\
0 \\
1
\end{array}\right] \\
&\mathbf{x}=s \mathbf{u}+t \mathbf{v}, \text { where } \mathbf{u}=\left[\begin{array}{l}
1 \\
1 \\
0
\end{array}\right], \quad \mathbf{v}=\left[\begin{array}{r}
-1 \\
0 \\
1
\end{array}\right] ; s, t \in \mathbb{C}, s \neq 0, t \neq 0
\end{aligned}
$$

So the eigenspace associated with the eigenvalue $-2$ is

$$
\mathcal{E}_{A}(-2)=\left\langle\left\{\left[\begin{array}{l}
1 \\
1 \\
0
\end{array}\right],\left[\begin{array}{r}
-1 \\
0 \\
1
\end{array}\right]\right\}\right\rangle
$$

</details>

Find the eigenvalues and eigenvectors of $A$.

$$
A=\left[\begin{array}{rr}
3 & -3 \\
1 & 1
\end{array}\right]
$$

<details><summary>Solution</summary>

Solve the characteristic equation to find the eigenvalues of $A$ :

$$
\begin{aligned}
|A-x I| &=0 \\
\left|\begin{array}{cc}
3-x & -3 \\
1 & 1-x
\end{array}\right| &=0 \\
(3-x)(1-x)+3 &=0 \\
x^{2}-4 x+6 &=0 \\
x &=2 \pm \sqrt{2} i
\end{aligned}
$$

Then solve the equation $(A-\lambda I) \mathbf{x}=0$ for each eigenvalue to find the corresponding eigenspace: For $\lambda=2+\sqrt{2} i$

$$
(A-\lambda I) \mathbf{x}=0
$$

$$
\left[\begin{array}{cc}
3-(2+\sqrt{2} i) & -3 \\
1 & 1-(2+\sqrt{2} i)
\end{array}\right] \mathbf{x}=0
$$

$$
\left[\begin{array}{cc}
1-\sqrt{2} i & -3 \\
1 & -1-\sqrt{2} i
\end{array}\right] \mathbf{x}=0
$$

Normally we would row-reduce the coefficient matrix to find a basis for the null space, but since this is a complex matrix, row-reducing is a pain so we use the fact that the matrix is singular to simplify things. Since $A-\lambda I$ is singular, the second row of the row-reduced form will be all zeros, meaning that both rows represent the same equation. We can therefore choose one of them to write the relationship between $x_{1}$ and $x_{2}$ :

$$
\begin{aligned}
1 x_{1}+(-1-\sqrt{2} i) x_{2} &=0 \\
x_{1} &=-(-1-\sqrt{2} i) x_{2} \\
x_{1} &=(1+\sqrt{2} i) x_{2}
\end{aligned}
$$

$\mathrm{So}$

$$
\mathbf{x}=\left[\begin{array}{l}
x_{1} \\
x_{2}
\end{array}\right]=\left[\begin{array}{c}
(1+\sqrt{2} i) x_{2} \\
x_{2}
\end{array}\right]=\left[\begin{array}{c}
1+\sqrt{2} i \\
1
\end{array}\right] x_{2}
$$

And the eigenspace corresponding to $\lambda=2+\sqrt{2} i$ is

$$
\mathcal{E}_{A}(2+\sqrt{2} i)=\left\langle\left\{\left[\begin{array}{c}
1+\sqrt{2} i \\
1
\end{array}\right]\right\}\right\rangle
$$

For $\lambda=2-\sqrt{2} i$

$$
(A-\lambda I) \mathbf{x}=0
$$

$$
\left[\begin{array}{cc}
3-(2-\sqrt{2} i) & -3 \\
1 & 1-(2-\sqrt{2} i)
\end{array}\right] \mathbf{x}=0
$$

$$
\left[\begin{array}{cc}
1+\sqrt{2} i & -3 \\
1 & -1+\sqrt{2} i
\end{array}\right] \mathbf{x}=0
$$

$$
\begin{aligned}
1 x_{1}+(-1+\sqrt{2} i) x_{2} &=0 \\
x_{1} &=-(-1+\sqrt{2} i) x_{2} \\
x_{1} &=(1-\sqrt{2} i) x_{2}
\end{aligned}
$$

So

$$
\mathbf{x}=\left[\begin{array}{l}
x_{1} \\
x_{2}
\end{array}\right]=\left[\begin{array}{c}
(1-\sqrt{2} i) x_{2} \\
x_{2}
\end{array}\right]=\left[\begin{array}{c}
1-\sqrt{2} i \\
1
\end{array}\right] x_{2}
$$

And the eigenspace corresponding to $\lambda=2-\sqrt{2} i$ is

$$
\mathcal{E}_{A}(2-\sqrt{2} i)=\left\langle\left\{\left[\begin{array}{c}
1-\sqrt{2} i \\
1
\end{array}\right]\right\}\right\rangle
$$

</details>

# Properties

## Definitions and Theorems {.tabset}

### Theorem 4.1.

Let $\A$ be a triangular matrix. Then the eigenvalues of $\A$ are its diagonal entries.

<details><summary>*Proof*</summary> 

Let $\A$ be an $n \times n$ (upper or lower) triangular matrix:

$$|\A − \lambda\I| = (d_1 − \lambda)(d_2 − \lambda)\dots(d_n − \lambda) = 0
\\ \text{then}
\\ \lambda_1 = d1
\\ \lambda_2 = d2
\\ \vdots
\\ \lambda_n = dn$$

</details> \newline 

### Theorem 4.2. 

If $\v_1, \v_2, \dots, \v_p$ are eigenvectors corresponding to distinct eigenvalues $\lambda_1, \lambda_2, \dots \lambda_p$, then the set $\{\v_1, \v_2, \dots, \v_p\}$ is linearly independent.

<details><summary>*Proof*</summary> 

$$\0 = (\A-\lambda_n\I_n)\0
\\ = (\A-\lambda_n\I_n)(\alpha_1\vx_1+\dots+\alpha_n\vx_n)
\\ = ((\A-\lambda_n\I_n)\alpha_1\vx_1+\dots+(\A-\lambda_k\I_n)\alpha_n\vx_n)
\\ = (\alpha_1(\A\vx_1-\lambda_n\I_n\vx_1)+\dots+\alpha_n(\A\vx_n-\lambda_n\I_n\vx_n))
\\ = (\alpha_1(\lambda_1\vx_1-\lambda_n\vx_1)+\dots+\alpha_n(\lambda_n\vx_n-\lambda_n\vx_n))
\\ = (\alpha_1(\lambda_1-\lambda_n)\vx_1+\dots+\alpha_n(\lambda_n-\lambda_n)\vx_n)
\\ = (\alpha_1(\lambda_1-\lambda_n)\vx_1+\dots+\alpha_n(\lambda_n-\lambda_n)\vx_n)
\\ = (\alpha_1(\lambda_1-\lambda_n)\vx_1+\dots+\alpha_{n-1}(\lambda_{n-1}-\lambda_n)\vx_{n-1} + \0)
\\ \Rightarrow \forall\ i, \alpha_i = 0$$

</details> \newline 

# Hermitian Matrices

## Definitions and Theorems {.tabset}

### Theorem 5.1. 

If $\A$ is a Hermitian matrix and $\lambda$ is an eigenvalue of $\A$, then $\lambda \in \R$.

<details><summary>*Proof*</summary> 

$$\lambda\innerProd{\v}{\v} = \innerProd{\v}{\lambda\v}
\\ = \innerProd{\v}{\A\v}
\\ = \innerProd{\A\v}{\v}
\\ = \innerProd{\lambda\v}{\v}
\\ = \bar\lambda\innerProd{\v}{\v}
\\ \Rightarrow \lambda = \bar \lambda$$

</details> \newline 

### Theorem 5.2.

If $\A$ is a Hermitian matrix, then any two eigenvectors from different eigenspaces are orthogonal.

<details><summary>*Proof*</summary> 

$$\lambda_1\innerProd{\v_1}{\v_2} = \innerProd{\A\v_1}{\v_2} = \innerProd{\v_1}{\A\v_2} = \innerProd{\v_1}{\lambda_2\v_2} = \lambda_2\innerProd{\v_1}{\v_2}
\\ \Rightarrow (\lambda_1 - \lambda_2)\innerProd{\v_1}{\v_2} = 0 \Rightarrow \innerProd{\v_1}{\v_2} = 0 $$

</details> \newline 







# Eigenvalues and Eigenvectors and Linear Transformations

## Definitions and Theorems {.tabset}

### Definition 2.2.
**Eigenvalues and Eigenvectors of Linear Transformations**

If $T: V \rightarrow V$ is a linear transformation then a nonzero vector $\mathbf{v} \in V$ is an eigenvector of $T$ associated with the eigenvalue $\lambda$ if

$$
T(\mathbf{v})=\lambda \mathbf{v}
$$

### Theorem 2.4. 
**Eigenvalues and Eigenvectors of Linear Transformations**

If $T: V \rightarrow V$ is a linear transformation and $B$ is a basis for $V$, then $\mathbf{v}$ is an eigenvector of $T$ corresponding to the eigenvalue $\lambda$ iff $\rho_{B}(\mathbf{v})$ is an eigenvector of $M_{B, B}^{T}$ corresponding to the eigenvalue $\lambda$.

## Examples

Find a basis $D$ for $\mathbb{P}_{2}$ composed of eigenvectors of the linear transformation $T: \mathbb{P}_{2} \rightarrow \mathbb{P}_{2}$. Then find the matrix representation for $T$ relative to that basis, $M_{D, D}^{T}$

$$
T\left(a+b x+c x^{2}\right)=(a+a)+(a+b) x+(a+c) x^{2}
$$
<details><summary>Solution</summary>

Let's choose an easy basis for $\mathbb{P}_{2}$ to work with to find the eigenvalues and eigenvectors of $T$ :

$$
B=\left\{1, x, x^{2}\right\}
$$

Then we can compute

$$
\begin{aligned}
&T(1)=T\left(1+0 x+0 x^{2}\right)=(1+1)+(1+0) x+(1+0) x^{2}=2+1 x+1 x^{2}=2+x+x^{2} \\
&T(x)=T\left(0+1 x+0 x^{2}\right)=(0+0)+(0+1) x+(0+0) x^{2}=0+1 x+0 x^{2}=x \\
&T\left(x^{2}\right)=T\left(0+0 x+1 x^{2}\right)=(0+0)+(0+0) x+(0+1) x^{2}=0+0 x+1 x^{2}=x^{2}
\end{aligned}
$$

By observation we can write

$$
\rho_{B}(T(1))=\left[\begin{array}{l}
2 \\
1 \\
1
\end{array}\right] \quad \rho_{B}(T(x))=\left[\begin{array}{l}
0 \\
1 \\
0
\end{array}\right] \quad \rho_{B}\left(T\left(x^{2}\right)\right)=\left[\begin{array}{l}
0 \\
0 \\
1
\end{array}\right]
$$

In general, the matrix representation for $T$ relative to $B$ and $C$ is

$$
M_{B, C}^{T}=\left[\rho_{C}\left(T\left(\mathbf{b}_{1}\right)\right)\left|\rho_{C}\left(T\left(\mathbf{b}_{2}\right)\right)\right| \cdots \mid \rho_{C}\left(T\left(\mathbf{b}_{n}\right)\right)\right]
$$

Therefore the matrix representation for $T$ relative to $B$ and $B$ (or simply relative to $B)$ is

$$
\begin{aligned}
M_{B, B}^{T} &=\left[\rho_{B}\left(T\left(\mathbf{b}_{1}\right)\right)\left|\rho_{B}\left(T\left(\mathbf{b}_{2}\right)\right)\right| \cdots \mid \rho_{B}\left(T\left(\mathbf{b}_{n}\right)\right)\right] \\
&=\left[\begin{array}{lll}
2 & 0 & 0 \\
1 & 1 & 0 \\
1 & 0 & 1
\end{array}\right]
\end{aligned}
$$

Now we can find the eigenvalues $M_{B, B}^{T}$, which will be the same as $T$.

To find eigenvalues we solve can use theorem 4.1:

$$
\lambda_{1}=2, \quad \lambda_{2}=1
$$

To find the eigenspace corresponding to $\lambda_{1}=2$, we solve

$$
\begin{gathered}
\left(M_{B, B}^{T}-2 I\right) \mathbf{x}=\mathbf{0} \quad \text { where } M_{B, B}^{T}=\left[\begin{array}{lll}
2 & 0 & 0 \\
1 & 1 & 0 \\
1 & 0 & 1
\end{array}\right] \\
M_{B, B}^{T}-2 I=\left[\begin{array}{rrr}
0 & 0 & 0 \\
1 & -1 & 0 \\
1 & 0 & -1
\end{array}\right] \sim\left[\begin{array}{rrr}
1 & 0 & -1 \\
0 & 1 & -1 \\
0 & 0 & 0
\end{array}\right] \\
x_{1}=x_{3} \\
x_{2}=x_{3} \\
x_{3}=\text { free }
\end{gathered}
$$

$$
\mathbf{x}=\left[\begin{array}{l}
x_{1} \\
x_{2} \\
x_{3}
\end{array}\right]=\left[\begin{array}{l}
x_{3} \\
x_{3} \\
x_{3}
\end{array}\right]=x_{3}\left[\begin{array}{l}
1 \\
1 \\
1
\end{array}\right], \quad \mathcal{E}_{M}(2)=\left\langle\left\{\left[\begin{array}{l}
1 \\
1 \\
1
\end{array}\right]\right\}\right\rangle, \quad \gamma_{M}(2)=1
$$

To find the eigenspace corresponding to $\lambda_{2}=1$, we solve

$$
\begin{gathered}
\left(M_{B, B}^{T}-1 I\right) \mathbf{x}=\mathbf{0} \quad \text { where } M_{B, B}^{T}=\left[\begin{array}{lll}
2 & 0 & 0 \\
1 & 1 & 0 \\
1 & 0 & 1
\end{array}\right] \\
M_{B, B}^{T}-I=\left[\begin{array}{lll}
1 & 0 & 0 \\
1 & 0 & 0 \\
1 & 0 & 0
\end{array}\right] \sim\left[\begin{array}{lll}
1 & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{array}\right] \\
x_{1}=0 \\
x_{2}=\text { free } \\
x_{3}=\text { free }
\end{gathered}
$$



$$
\mathbf{x}=\left[\begin{array}{l}
x_{1} \\
x_{2} \\
x_{3}
\end{array}\right]=\left[\begin{array}{c}
0 \\
x_{2} \\
x_{3}
\end{array}\right]=x_{2}\left[\begin{array}{l}
0 \\
1 \\
0
\end{array}\right]+x_{3}\left[\begin{array}{l}
0 \\
0 \\
1
\end{array}\right] \quad \mathcal{E}_{M}(1)=\left\langle\left\{\left[\begin{array}{l}
0 \\
1 \\
0
\end{array}\right],\left[\begin{array}{l}
0 \\
0 \\
1
\end{array}\right]\right\}\right\rangle, \quad \gamma_{M}(1)=2
$$

Now to find the eigenvectors of $T$, we use the inverse transformation $\rho_{B}^{-1}$ on these eigenvectors of $M_{B, B}^{T}$ :

$$
\begin{aligned}
&\rho_{B}^{-1}\left(\left[\begin{array}{l}
1 \\
1 \\
1
\end{array}\right]\right)=1 \cdot 1+1 x+1 x^{2}=1+x+x^{2} \\
&\rho_{B}^{-1}\left(\left[\begin{array}{l}
0 \\
1 \\
0
\end{array}\right]\right)=0 \cdot 1+1 x+0 x^{2}=x \\
&\rho_{B}^{-1}\left(\left[\begin{array}{l}
0 \\
0 \\
1
\end{array}\right]\right)=0 \cdot 1+0 x+1 x^{2}=x^{2}
\end{aligned}
$$

This gives the eigenvectors of $T$ that form a basis for $\mathbb{P}_{2}$.

$$
D=\left\{1+x+x^{2}, x, x^{2}\right\}
$$

We know this set is a basis because the set of coordinate vectors forms a basis for $\mathbb{C}^{3}$, as we can see by constructing a matrix with the vectors as its columns and row reducing:

$$
\left[\begin{array}{lll}
1 & 0 & 0 \\
1 & 1 & 0 \\
1 & 0 & 0
\end{array}\right] \sim\left[\begin{array}{lll}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{array}\right]
$$

Now to find $M_{D, D}^{T}$ we repeat the steps we used to find $M_{B, B}^{T}$ with the new basis D. First we find the images of the basis vectors under $T$, and fortunately, two of the basis vectors in $D$ are the same as two of the basis vectors in $B$, so we can just recycle that work: 

$$
\begin{aligned}
T(x)=T\left(0+1 x+0 x^{2}\right) &=(0+0)+(0+1) x+(0+0) x^{2}=x \\
T\left(x^{2}\right)=T\left(0+0 x+1 x^{2}\right) &=(0+0)+(0+0) x+(0+1) x^{2}=x^{2} \\
T\left(1+x+x^{2}\right) &=(1+1)+(1+1) x+(1+1) x^{2}=2+2 x+2 x^{2}
\end{aligned}
$$

Now we want the coordinate vectors of these polynomials relative to $D$. We use the definition of the coordinate vectors to figure them out:

$$
\rho_{D}(x)=\left[\begin{array}{l}
c_{1} \\
c_{2} \\
c_{3}
\end{array}\right]
$$

where

$$
x=c_{1} x+c_{2} x^{2}+c_{3}\left(1+x+x^{2}\right)
$$

Normally we'd have to solve a system of equations to figure out $c_{1}, c_{2}$, and $c_{3}$. However in this case we get lucky if we notice that $x$ is itself one of the basis vectors in $D$ so we can just let

$$
\begin{aligned}
&c_{1}=1 \\
&c_{2}=0 \\
&c_{3}=0
\end{aligned}
$$

to get

$$
x=c_{1} x+c_{2} x^{2}+c_{3}\left(1+x+x^{2}\right)=1 x+0 x^{2}+0\left(1+x+x^{2}\right)=x
$$

Therefore

$$
\rho_{D}(x)=\left[\begin{array}{l}
1 \\
0 \\
0
\end{array}\right]
$$

Similarly, to get $\rho_{D}\left(x^{2}\right)$ we solve

$$
x^{2}=c_{1} x+c_{2} x^{2}+c_{3}\left(1+x+x^{2}\right)
$$

and again we get lucky since we can observe that

$$
\begin{aligned}
&c_{1}=0 \\
&c_{2}=1 \\
&c_{3}=0
\end{aligned}
$$

to get

$$
x=c_{1} x+c_{2} x^{2}+c_{3}\left(1+x+x^{2}\right)=0 x+1 x^{2}+0\left(1+x+x^{2}\right)=x^{2}
$$

Therefore

$$
\rho_{D}\left(x^{2}\right)=\left[\begin{array}{l}
0 \\
1 \\
0
\end{array}\right]
$$

Finally, to find $\rho_{D}\left(2+2 x+2 x^{2}\right)$ we solve

$$
2+2 x+2 x^{2}=c_{1} x+c_{2} x^{2}+c_{3}\left(1+x+x^{2}\right)
$$

and again by observation

$$
\begin{aligned}
&c_{1}=0 \\
&c_{2}=0 \\
&c_{3}=2
\end{aligned}
$$

gives

$$
x=c_{1} x+c_{2} x^{2}+c_{3}\left(1+x+x^{2}\right)=0 x+0 x^{2}+2\left(1+x+x^{2}\right)=2+2 x+2 x^{2}
$$

Therefore

$$
\rho_{D}\left(2+2 x+2 x^{2}\right)=\left[\begin{array}{l}
0 \\
0 \\
2
\end{array}\right]
$$

Putting it all together, we construct the matrix representation for $T$ relative to $D:$

$$
\begin{aligned}
M_{D, D}^{T} &=\left[\rho_{D}(T(x))\left|\rho_{D}\left(T\left(x^{2}\right)\right)\right| \rho_{D}\left(T\left(1+1 x+1 x^{2}\right)\right)\right] \\
&=\left[\begin{array}{lll}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 2
\end{array}\right]
\end{aligned}
$$

Notice that $M_{D, D}^{T}$ is a diagonal matrix, and the diagonal entries are the eigenvalues of $T$ (which were also the the eigenvalues of $M_{B, B}^{T}$ ). This is not a coincidence, and we will talk more about this nex class.

</details>
