---
title: "Least Squares"
author: "Brody Erlandson"
output:
  html_document:
    code_folding: hide
    toc: true
    toc_float: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Orthogonal Projections

## Definitions and Theorems {.tabset}

### Theorem 1.1.
**Orthogonal Basis Representation**

If $\left\{\mathbf{u}_{1}, \mathbf{u}_{2}, \ldots, \mathbf{u}_{n}\right\}$ is an orthogonal basis for an inner product space $V$, then for any vector $\mathbf{y}$ in $V$

$$
\mathbf{y}=\sum_{i=1}^{n} \frac{\left\langle\mathbf{u}_{i}, \mathbf{y}\right\rangle}{\left\langle\mathbf{u}_{i}, \mathbf{u}_{i}\right\rangle} \mathbf{u}_{i}
$$

<details><summary>Proof</summary> Write $\mathbf{y}$ as a linear combination of the basis vectors as

$$
\mathbf{y}=\sum_{i=1}^{n} c_{i} \mathbf{u}_{i}
$$

Now take the inner product of the basis vector $\mathbf{u}_{j}$ with $\mathbf{y}$ :

$$
\begin{aligned}
\left\langle\mathbf{u}_{j}, \mathbf{y}\right\rangle &=\left\langle\mathbf{u}_{j}, \sum_{i=1}^{n} c_{i} \mathbf{u}_{i}\right\rangle \\
\left\langle\mathbf{u}_{j}, \mathbf{y}\right\rangle &=\sum_{i=1}^{n} c_{i}\left\langle\mathbf{u}_{j}, \mathbf{u}_{i}\right\rangle \\
\left\langle\mathbf{u}_{j}, \mathbf{y}\right\rangle &=c_{1}\left\langle\mathbf{u}_{j}, \mathbf{u}_{1}\right\rangle+c_{2}\left\langle\mathbf{u}_{j}, \mathbf{u}_{2}\right\rangle+\cdots+c_{j}\left\langle\mathbf{u}_{j}, \mathbf{u}_{j}\right\rangle+\cdots+c_{n}\left\langle\mathbf{u}_{j}, \mathbf{u}_{n}\right\rangle \\
\left\langle\mathbf{u}_{j}, \mathbf{y}\right\rangle &=0+0+\cdots+c_{j}\left\langle\mathbf{u}_{j}, \mathbf{u}_{j}\right\rangle+0+\cdots+0 \\
c_{j} &=\frac{\left\langle\mathbf{u}_{j}, \mathbf{y}\right\rangle}{\left\langle\mathbf{u}_{j}, \mathbf{u}_{j}\right\rangle}
\end{aligned}
$$

Since $j$ was arbitrary, we have

$$
c_{1}=\frac{\left\langle\mathbf{u}_{1}, \mathbf{y}\right\rangle}{\left\langle\mathbf{u}_{1}, \mathbf{u}_{1}\right\rangle}, \quad c_{2}=\frac{\left\langle\mathbf{u}_{2}, \mathbf{y}\right\rangle}{\left\langle\mathbf{u}_{2}, \mathbf{u}_{2}\right\rangle}, \quad c_{3}=\frac{\left\langle\mathbf{u}_{3}, \mathbf{y}\right\rangle}{\left\langle\mathbf{u}_{3}, \mathbf{u}_{3}\right\rangle}, \quad \ldots \quad c_{n}=\frac{\left\langle\mathbf{u}_{n}, \mathbf{y}\right\rangle}{\left\langle\mathbf{u}_{n}, \mathbf{u}_{n}\right\rangle}
$$

therefore

$$
\mathbf{y}=\sum_{i=1}^{n} c_{i} \mathbf{u}_{i}=\sum_{i=1}^{n} \frac{\left\langle\mathbf{u}_{i}, \mathbf{y}\right\rangle}{\left\langle\mathbf{u}_{i}, \mathbf{u}_{i}\right\rangle} \mathbf{u}_{i}
$$

</details>

### Definition 1.1.
**Orthogonal Projection**

Let $\mathbf{y}$ be a vector in a vector space $V$, let

$$
\left\{\mathbf{u}_{1}, \mathbf{u}_{2}, \ldots, \mathbf{u}_{n}\right\}
$$

be an orthogonal basis for $V$, and

$$
W=\left\langle\left\{\mathbf{u}_{1}, \mathbf{u}_{2}, \ldots, \mathbf{u}_{p}\right\}\right\rangle
$$

where $p<n$.

The orthogonal projection of $\mathbf{y}$ onto $W$ is

$$
\hat{\mathbf{y}}=\operatorname{proj}_{W} \mathbf{y}=\sum_{i=1}^{p} \frac{\left\langle\mathbf{u}_{i}, \mathbf{y}\right\rangle}{\left\langle\mathbf{u}_{i}, \mathbf{u}_{i}\right\rangle} \mathbf{u}_{i}
$$

### Definition 1.2.
**Orthogonal Complement**

Let $\mathbf{y}$ be a vector in a vector space $V$, let

$$
\left\{\mathbf{u}_{1}, \mathbf{u}_{2}, \ldots, \mathbf{u}_{n}\right\}
$$

be an orthogonal basis for $V$, and

$$
W=\left\langle\left\{\mathbf{u}_{1}, \mathbf{u}_{2}, \ldots, \mathbf{u}_{p}\right\}\right\rangle
$$

where $p<n$.

The orthogonal complement of $W$, written $W^{\perp}$, is the set of all vectors in $V$ that are orthogonal to every vector in $W$ :

$$
W^{\perp}=\{\mathbf{y} \in V: \mathbf{x} \perp \mathbf{y}, \forall \mathbf{x} \in W\}
$$

Recall that $\mathbf{x} \perp \mathbf{y}$ means $\mathbf{x}$ is orthogonal to $\mathbf{y}$, which means

$$
\langle\mathbf{x}, \mathbf{y}\rangle=0
$$

### Theorem 1.2.
**Orthogonal Decomposition Theorem**

If $W$ is a subspace of $\mathbb{R}^{n}$ and

$$
\left\{\mathbf{u}_{1}, \mathbf{u}_{2}, \ldots, \mathbf{u}_{p}\right\}
$$

is an orthogonal basis for $W$, then each vector $\mathbf{y} \in \mathbb{R}^{n}$ can be written uniquely in the form:

$$
\mathbf{y}=\hat{\mathbf{y}}+\mathbf{z}
$$

where $\hat{\mathbf{y}}=\operatorname{proj}_{W} \mathbf{y}$ and $\mathbf{z} \in W^{\perp}$.

<details><summary>Proof</summary>

Let

$$
\left\{\mathbf{u}_{1}, \mathbf{u}_{2}, \ldots, \mathbf{u}_{n}\right\}
$$

be an orthogonal basis for $\mathbb{R}^{n}$ and let

$$
W=\left\langle\left\{\mathbf{u}_{1}, \mathbf{u}_{2}, \ldots, \mathbf{u}_{p}\right\}\right\rangle
$$

where $p<n$. For notational simplicity, let

$$
\frac{\mathbf{u}_{i} \cdot \mathbf{y}}{\mathbf{u}_{i} \cdot \mathbf{u}_{i}}=a_{i}
$$

Then by theorem $1.1$ we can write $\mathbf{y}$ uniquely as

$$
\begin{aligned}
\mathbf{y} &=\sum_{i=1}^{n} \frac{\mathbf{u}_{i} \cdot \mathbf{y}}{\mathbf{u}_{i} \cdot \mathbf{u}_{i}} \mathbf{u}_{i}=\sum_{i=1}^{n} a_{i} \mathbf{u}_{i} \\
&=a_{1} \mathbf{u}_{1}+a_{2} \mathbf{u}_{2}+\cdots+a_{p} \mathbf{u}_{p}+a_{p+1} \mathbf{u}_{p+1}+\cdots+a_{n} \mathbf{u}_{n} \\
&=\sum_{i=1}^{p} a_{i} \mathbf{u}_{i}+\sum_{i=p+1}^{n} a_{i} \mathbf{u}_{i} \\
&=\operatorname{proj}_{W} \mathbf{y}+\sum_{i=p+1}^{n} a_{i} \mathbf{u}_{i}
\end{aligned}
$$

Let

$$
\mathbf{z}=\sum_{i=p+1}^{n} a_{i} \mathbf{u}_{i}
$$

and we have

$$
y=\hat{y}+z
$$

To see that $\mathbf{z} \in W^{\perp}$, take the dot product of $\mathbf{z}$ with an arbitrary vector in $W$.

Let

$$
\mathbf{x}=\sum_{i=1}^{p} c_{i} \mathbf{u}_{i}
$$

be an arbitrary vector in $W$. Then 

$$
\begin{aligned}
\mathbf{z} \cdot \mathbf{x}=&\left(\sum_{i=p+1}^{n} a_{i} \mathbf{u}_{i}\right) \cdot\left(\sum_{i=1}^{p} c_{i} \mathbf{u}_{i}\right) \\
=&\left(a_{p+1} \mathbf{u}_{p+1}+a_{p+2} \mathbf{u}_{p+2}+\cdots+a_{n} \mathbf{u}_{n}\right) \cdot\left(c_{1} \mathbf{u}_{1}+c_{2} \mathbf{u}_{2}+\cdots+c_{p} \mathbf{u}_{p}\right) \\
=& a_{p+1} \mathbf{u}_{p+1} \cdot\left(c_{1} \mathbf{u}_{1}+c_{2} \mathbf{u}_{2}+\cdots+c_{p} \mathbf{u}_{p}\right)+a_{p+2} \mathbf{u}_{p+2} \cdot\left(c_{1} \mathbf{u}_{1}+c_{2} \mathbf{u}_{2}+\cdots+c_{p} \mathbf{u}_{p}\right)+\cdots \\
& \quad+a_{n} \mathbf{u}_{n} \cdot\left(c_{1} \mathbf{u}_{1}+c_{2} \mathbf{u}_{2}+\cdots+c_{p} \mathbf{u}_{p}\right) \\
=& a_{p+1} \mathbf{u}_{p+1} \cdot c_{1} \mathbf{u}_{1}+a_{p+1} \mathbf{u}_{p+1} \cdot c_{2} \mathbf{u}_{2}+\cdots+a_{p+1} \mathbf{u}_{p+1} \cdot c_{p} \mathbf{u}_{p} \\
& \quad+a_{p+2} \mathbf{u}_{p+2} \cdot c_{1} \mathbf{u}_{1}+a_{p+2} \mathbf{u}_{p+2} \cdot c_{2} \mathbf{u}_{2}+\cdots+a_{p+2} \mathbf{u}_{p+2} \cdot c_{p} \mathbf{u}_{p}+\cdots \\
&+a_{n} \mathbf{u}_{n} \cdot c_{1} \mathbf{u}_{1}+a_{n} \mathbf{u}_{n} \cdot c_{2} \mathbf{u}_{2}+\cdots+a_{n} \mathbf{u}_{n} \cdot c_{p} \mathbf{u}_{p} \\
=&\left(a_{p+1} c_{1}\right) \mathbf{u}_{p+1} \cdot \mathbf{u}_{1}+\left(a_{p+1} c_{2}\right) \mathbf{u}_{p+1} \cdot \mathbf{u}_{2}+\cdots+\left(a_{p+1} c_{p}\right) \mathbf{u}_{p+1} \cdot \mathbf{u}_{p} \\
& \quad+\left(a_{p+2} c_{1}\right) \mathbf{u}_{p+2} \cdot \mathbf{u}_{1}+\left(a_{p+2} c_{2}\right) \mathbf{u}_{p+2} \cdot \mathbf{u}_{2}+\cdots+\left(a_{p+2} c_{p}\right) \mathbf{u}_{p+2} \cdot \mathbf{u}_{p}+\cdots \\
& \quad+\quad\left(a_{n} c_{1}\right) \mathbf{u}_{n} \cdot \mathbf{u}_{1}+\left(a_{n} c_{2}\right) \mathbf{u}_{n} \cdot \mathbf{u}_{2}+\cdots+\left(a_{n} c_{p}\right) \mathbf{u}_{n} \cdot \mathbf{u}_{p} \\
=& \sum_{i=p+1}^{n} \sum_{j=1}^{p}\left(a_{i} c_{j}\right) \mathbf{u}_{i} \cdot \mathbf{u}_{j} \\
=&\ 0 
\end{aligned}
$$

since

$$
\mathbf{u}_{i} \cdot \mathbf{u}_{j}=0
$$

for $i \neq j$

</details>

### Theorem 1.3.
**Pythagorean Theorem**

If $\mathbf{u}$ and $\mathbf{v}$ are in $\mathbb{R}^{n}$, and

$$
\mathbf{u} \perp \mathbf{v}
$$

then

$$
\|\mathbf{u}+\mathbf{v}\|^{2}=\|\mathbf{u}\|^{2}+\|\mathbf{v}\|^{2}
$$

<details><summary>*Proof*</summary>

$$
\begin{aligned}
\|\mathbf{u}+\mathbf{v}\|^{2} &=(\mathbf{u}+\mathbf{v}) \cdot(\mathbf{u}+\mathbf{v}) \\
&=\mathbf{u} \cdot \mathbf{u}+\mathbf{u} \cdot \mathbf{v}+\mathbf{v} \cdot \mathbf{u}+\mathbf{v} \cdot \mathbf{v} \\
&=\mathbf{u} \cdot \mathbf{u}+2 \mathbf{u} \cdot \mathbf{v}+\mathbf{v} \cdot \mathbf{v} \\
&=\mathbf{u} \cdot \mathbf{u}+\mathbf{v} \cdot \mathbf{v} \\
&=\|\mathbf{u}\|^{2}+\|\mathbf{v}\|^{2}
\end{aligned}
$$

</details>

### Theorem 1.4.
**Best Approximation Theorem**

If $W$ is a subspace of $\mathbb{R}^{n}, \mathbf{y} \in \mathbb{R}^{n}$, and $\hat{\mathbf{y}}=\operatorname{proj}_{W} \mathbf{y}$, then

$$
\|\mathbf{y}-\hat{\mathbf{y}}\|<\|\mathbf{y}-\mathbf{v}\|
$$

for all $\mathbf{v} \in W, \mathbf{v} \neq \hat{\mathbf{y}}$

![](projToSubspace.jpg)

<details><summary>*Proof*</summary> 

By theorem $1.2$

$$
\mathbf{y}=\hat{\mathbf{y}}+\mathbf{z}
$$

where

$$
\hat{\mathbf{y}} \in W \quad \text { and } \quad \mathbf{z} \in W^{\perp}
$$

Since

$$
\mathbf{z}=\mathbf{y}-\hat{\mathbf{y}}
$$

this means

$$
\mathbf{y}-\hat{\mathbf{y}} \in W^{\perp}
$$

Let $\mathbf{v}$ be a vector in $W$ distinct from $\hat{\mathbf{y}}$. Since $\hat{\mathbf{y}}$ is in $W$, and $W$ is a subspace,

$$
\hat{\mathbf{y}}-\mathbf{v} \in W
$$

Therefore $\mathbf{y}-\hat{\mathbf{y}}$ and $\hat{\mathbf{y}}-\mathbf{v}$ are orthogonal to each other.

Now we use the Pythagorean Theorem. First off,

$$
(\mathbf{y}-\hat{\mathbf{y}})+(\hat{\mathbf{y}}-\mathbf{v})=\mathbf{y}-\mathbf{v}
$$

Then since the two vectors on the left side of this equation are orthogonal, by the Pythagorean Theorem,

$$
\|\mathbf{y}-\hat{\mathbf{y}}\|^{2}+\|\hat{\mathbf{y}}-\mathbf{v}\|^{2}=\|\mathbf{y}-\mathbf{v}\|^{2}
$$

Since $\hat{\mathbf{y}} \neq \mathbf{v}, \hat{\mathbf{y}}-\mathbf{v} \neq 0$, and

$$
\|\hat{\mathbf{y}}-\mathbf{v}\|^{2}>0
$$

we have

$$
\|\mathbf{y}-\hat{\mathbf{y}}\|^{2}<\|\mathbf{y}-\mathbf{v}\|^{2}
$$

And since the norms above are non-negative, we have the desired result

$$
\|\mathbf{y}-\hat{\mathbf{y}}\|<\|\mathbf{y}-\mathbf{v}\|
$$

</details>

## Examples

Find the orthogonal projection of the vector $y$ onto the vector $\mathbf{u}$ below.

![](project.jpg)

<details><summary>Solution</summary>

Let's call the orthogonal projection $\hat{\mathbf{y}}$. Note that $\hat{\mathbf{y}}$ is in the same direction as $\mathbf{u}$, and is therefore orthogonal (i.e. perpendicular) to a vector like $\mathbf{z}$ that is orthogonal to $\mathbf{u}$. We'll use this fact to figure out a formula for $\hat{\mathbf{y}}$.

Since $\hat{\mathbf{y}}$ is in the same direction as $\mathbf{u}$, we can write

$$
\hat{\mathbf{y}}=c \mathbf{u}
$$

for some scalar $c$. Then since $\mathbf{u}$ is orthogonal to $\mathbf{z}$, we can write

$$
\mathbf{u} \cdot \mathbf{z}=0
$$

From the parallelogram rule for adding vectors in $\mathbb{R}^{2}$, we can see that

$$
\mathbf{y}=\hat{\mathbf{y}}+\mathbf{z} \quad \Longrightarrow \quad \mathbf{z}=\mathbf{y}-\hat{\mathbf{y}}
$$

Putting this all together, we get

$$
\begin{aligned}
\mathbf{u} \cdot \mathbf{z} &=0 \\
\mathbf{u} \cdot(\mathbf{y}-\hat{\mathbf{y}}) &=0 \\
\mathbf{u} \cdot(\mathbf{y}-c \mathbf{u}) &=0 \\
\mathbf{u} \cdot \mathbf{y}-\mathbf{u} \cdot c \mathbf{u} &=0 \\
-c \mathbf{u} \cdot \mathbf{u} &=-\mathbf{u} \cdot \mathbf{y} \\
c &=\frac{\mathbf{u} \cdot \mathbf{y}}{\mathbf{u} \cdot \mathbf{u}}
\end{aligned}
$$

Substituting for $c$ we solve for $\hat{\mathbf{y}}$ :

$$
\begin{aligned}
&\hat{\mathbf{y}}=c \mathbf{u} \\
&\hat{\mathbf{y}}=\left(\frac{\mathbf{u} \cdot \mathbf{y}}{\mathbf{u} \cdot \mathbf{u}}\right) \mathbf{u}
\end{aligned}
$$

We can also use definition 1.1 directly.

</details>

Decompose the vector $y$ into orthogonal components in the directions of the standard basis vectors for $\mathbb{R}^{3}$. Then find the orthogonal projection of $\mathbf{y}$ onto the $x_{1}-x_{2}$ plane.

$$
\mathbf{y}=\left[\begin{array}{l}
3 \\
5 \\
8
\end{array}\right]
$$

<details><summary>Solution</summary>

![](3dvect.jpg)

The standard basis for $\mathbb{R}^{3}$ is

$$
\left\{\mathbf{e}_{1}, \mathbf{e}_{2}, \mathbf{e}_{3}\right\}
$$

where

$$
\mathbf{e}_{1}=\left[\begin{array}{l}
1 \\
0 \\
0
\end{array}\right], \quad \mathbf{e}_{2}=\left[\begin{array}{l}
0 \\
1 \\
0
\end{array}\right], \quad \mathbf{e}_{3}=\left[\begin{array}{l}
0 \\
0 \\
1
\end{array}\right]
$$

The orthogonal projection of $\mathbf{y}$ onto $\mathbf{e}_{1}$ is

$$
\hat{\mathbf{y}}_{1}=\frac{\mathbf{e}_{1} \cdot \mathbf{y}}{\mathbf{e}_{1} \cdot \mathbf{e}_{1}} \mathbf{e}_{1}=\left(\frac{1 \cdot 3+0 \cdot 5+0 \cdot 8}{1 \cdot 1+0 \cdot 0+0 \cdot 0}\right)\left[\begin{array}{l}
1 \\
0 \\
0
\end{array}\right]=\left[\begin{array}{l}
3 \\
0 \\
0
\end{array}\right]
$$

Likewise

$$
\hat{\mathbf{y}}_{2}=\frac{\mathbf{e}_{2} \cdot \mathbf{y}}{\mathbf{e}_{2} \cdot \mathbf{e}_{2}} \mathbf{e}_{2}=\left(\frac{0 \cdot 3+1 \cdot 5+0 \cdot 8}{0 \cdot 0+1 \cdot 1+0 \cdot 0}\right)\left[\begin{array}{l}
0 \\
1 \\
0
\end{array}\right]=\left[\begin{array}{l}
0 \\
5 \\
0
\end{array}\right]
$$

and

$$
\hat{\mathbf{y}}_{3}=\frac{\mathbf{e}_{3} \cdot \mathbf{y}}{\mathbf{e}_{3} \cdot \mathbf{e}_{3}} \mathbf{e}_{3}=\left(\frac{0 \cdot 3+0 \cdot 5+1 \cdot 8}{0 \cdot 0+0 \cdot 0+1 \cdot 1}\right)\left[\begin{array}{l}
0 \\
0 \\
1
\end{array}\right]=\left[\begin{array}{l}
0 \\
0 \\
8
\end{array}\right]
$$

We can see all these vectors in the diagram below. In addition, we can see the orthogonal projection of $\mathbf{y}$ onto the $x_{1}-x_{2}$ plane, which is the green vector labeled $\hat{\mathbf{y}}$. Notice that this vector is just the sum of the two projections of $\mathbf{y}$ onto the $x_{1}$ and $x_{2}$ axes, or more accurately, onto $\mathbf{e}_{1}$ and $\mathbf{e}_{2}$ :

$$
\hat{\mathbf{y}}=\hat{\mathbf{y}}_{1}+\hat{\mathbf{y}}_{2}=\left[\begin{array}{l}
3 \\
0 \\
0
\end{array}\right]+\left[\begin{array}{l}
0 \\
5 \\
0
\end{array}\right]=\left[\begin{array}{l}
3 \\
5 \\
0
\end{array}\right]
$$

![](3dvectProjs.jpg)

Notice also that, as in theorem $1.2, \mathbf{y}$ is equal to the sum of the projections onto all the axes:

$$
\mathbf{y}=\hat{\mathbf{y}}_{1}+\hat{\mathbf{y}}_{2}+\hat{\mathbf{y}}_{3}
$$

</details>

Let $W$ be the $x_{1}-x_{2}$ plane in $\mathbb{R}^{3}$. Find $W^{\perp}$.

<details><summary>Solution</summary>

We can write the $x_{1}-x_{2}$ plane as the span of the first two standard basis vectors for $\mathbb{R}^{3}$ :

$$
W=\left\langle\left\{\mathbf{e}_{1}, \mathbf{e}_{2}\right\}\right\rangle
$$

where

$$
\mathbf{e}_{1}=\left[\begin{array}{l}
1 \\
0 \\
0
\end{array}\right], \quad \mathbf{e}_{2}=\left[\begin{array}{l}
0 \\
1 \\
0
\end{array}\right]
$$

Geometrically, we can picture what the vectors in $W^{\perp}$ will look like. All the colored vectors in the figure below are in $W^{\perp}$. They are all "vertical", or more specifically, parallel to the $x_{3}$ axis. 

![](x3ParrArr.jpg)

Algebraically, we can write an arbitrary vector in $W$ as a linear combination of the vectors in the spanning set for $W$. For any $\mathbf{y} \in W$,

$$
\mathbf{y}=a \mathbf{e}_{1}+b \mathbf{e}_{2}=a\left[\begin{array}{l}
1 \\
0 \\
0
\end{array}\right]+b\left[\begin{array}{l}
0 \\
1 \\
0
\end{array}\right]=\left[\begin{array}{l}
a \\
b \\
0
\end{array}\right]
$$

for some real numbers $a$ and $b$. Since every vector

$$
\mathbf{x}=\left[\begin{array}{l}
x_{1} \\
x_{2} \\
x_{3}
\end{array}\right]
$$

in $W^{\perp}$ is orthogonal to every vector $\mathbf{y}$ in $W$, we have 

$$
\begin{aligned}
\mathbf{x} \cdot \mathbf{y} &=0 \\
{\left[\begin{array}{l}
x_{1} \\
x_{2} \\
x_{3}
\end{array}\right] \cdot\left[\begin{array}{l}
a \\
b \\
0
\end{array}\right] } &=0 \\
x_{1} a+x_{2} b+x_{3} 0 &=0 \\
x_{1} a+x_{2} b &=0
\end{aligned}
$$

Since this equation has to be true for all possible values of $a$ and $b$, the only solution is

$$
x_{1}=x_{2}=0
$$

$x_{3}$, however is free; it can be any real number. So

$$
\mathbf{x}=\left[\begin{array}{c}
0 \\
0 \\
x_{3}
\end{array}\right]=x_{3}\left[\begin{array}{l}
0 \\
0 \\
1
\end{array}\right]=c \mathbf{e}_{3}
$$

for some real number $c . W^{\perp}$ is set of all vectors of this form, or the span of the set containing $\mathbf{e}_{3}$ :

$$
W^{\perp}=\left\langle\left\{\mathbf{e}_{3}\right\}\right\rangle
$$

This is just the $x_{3}$ axis in the figure above.

</details>

# Least Squares Solution

Previously if a matrix equation (or a system of equations) was inconsistent, we left it at that. Now we want to dig a little deeper. Consider an inconsistent matrix equation

$$
A \mathbf{x}=\mathbf{b}
$$

Recall, "inconsistent" means given the matrix $A$ and the vector $\mathbf{b}$, there is no vector $\mathbf{x}$ that satisfies this equation. One way to interpret this is that $\mathbf{b}$ is not in $\mathcal{C}(A)$, since the matrix equation is saying that $\mathbf{b}$ is a linear combination of the columns of $A$ with the entries of $\mathbf{x}$ as the coefficients. Therefore if the equation is inconsistent, there is no $\mathbf{x}$, there are no coefficients, and $\mathbf{b}$ cannot be written as linear combination of the columns of $A$.

What we want to consider now is, what vector that is in $\mathcal{C}(A)$ is closest to $\mathbf{b}$ that isn't in $\mathcal{C}(A) ?$

![](projToSubspaceOLS.jpg)

From the Best Approximation Theorem (thm 1.4) we know that $\hat{\mathbf{b}}$ is the closest vector in $\mathcal{C}(A)$ to $\mathbf{b}$. 

## Definitions and Theorems {.tabset}

### Definition 2.1.
**Least Squares Solution**

If $A$ is an $m \times n$ matrix and $\mathbf{b} \in \mathbb{R}^{m}$, the least squares solution of $A \mathbf{x}=\mathbf{b}$ is $\hat{\mathbf{x}} \in \mathbb{R}^{n}$ such that $A \hat{\mathbf{x}}=\hat{\mathbf{b}}$, and

$$
\|\mathbf{b}-\hat{\mathbf{b}}\| \leq\|\mathbf{b}-A \mathbf{x}\| \quad \forall \mathbf{x} \in \mathbb{R}^{n}
$$

To find the least squares solution to any equation $A \mathbf{x}=\mathbf{b}$, we can use the Orthogonal Decomposition Theorem (thm 1.2), translated to our current context.

### Theorem 2.1.
**The Orthogonal Decomposition Theorem for $\mathcal{C}(A)$**

Let $A$ be an $m \times n$ matrix and let $\mathbf{b}$ be in $\mathbb{R}^{m}$. Then $\mathbf{b}$ can be written uniquely in the form

$$
\mathbf{b}=\hat{\mathbf{b}}+\mathbf{z}
$$

where $\hat{\mathbf{b}}=\operatorname{proj}_{\mathcal{C}(A)} \mathbf{b}$ and $\mathbf{z} \in \mathcal{C}(A)^{\perp}$

Recall that

$$
\mathbf{z}=\mathbf{b}-\hat{\mathbf{b}}
$$

is orthogonal to every vector in $\mathcal{C}(A)$, including every column of $A$.

In other words, if

$$
A=\left[\begin{array}{l|l|l|l}
\mathbf{a}_{1} & \mathbf{a}_{2} & \cdots & \mathbf{a}_{n}
\end{array}\right]
$$

then

$$
\mathbf{b}-\hat{\mathbf{b}} \perp \operatorname{Col} A
$$

and

$$
\mathbf{a}_{i} \cdot(\mathbf{b}-\hat{\mathbf{b}})=0, i=1,2, \ldots, n
$$

Using the standard trick of writing $\mathbf{u} \cdot \mathbf{v}$ as $\mathbf{u}^{T} \mathbf{v}$, we can write the following: 

$$
\begin{aligned}
\mathbf{a}_{i} \cdot(\mathbf{b}-\hat{\mathbf{b}}) &=0 \\
\mathbf{a}_{i}^{T}(\mathbf{b}-\hat{\mathbf{b}}) &=0 \\
\mathbf{a}_{i}^{T}(\mathbf{b}-A \hat{\mathbf{x}}) &=0 \\
A^{T}(\mathbf{b}-A \hat{\mathbf{x}}) &=\mathbf{0} \\
A^{T} \mathbf{b}-A^{T} A \hat{\mathbf{x}} &=\mathbf{0} \\
A^{T} \mathbf{b} &=A^{T} A \hat{\mathbf{x}}
\end{aligned}
$$

This last equation is called the normal equations for $A \mathbf{x}=\mathbf{b}$. We solve the normal equations to find the least squares solution $\hat{\mathbf{x}}$.

## Examples

Find the least squares solution to the inconsistent system $A \mathrm{x}=$ b where

$$
A=\left[\begin{array}{ll}
4 & 0 \\
0 & 2 \\
1 & 1
\end{array}\right], \quad \mathbf{b}=\left[\begin{array}{c}
2 \\
0 \\
11
\end{array}\right]
$$

<details><summary>Solution</summary>

We solve the normal equations:

$$
\begin{gathered}
A^{T} A \hat{\mathbf{x}}=A^{T} \mathbf{b} \\
A^{T} A=\left[\begin{array}{lll}
4 & 0 & 1 \\
0 & 2 & 1
\end{array}\right]\left[\begin{array}{ll}
4 & 0 \\
0 & 2 \\
1 & 1
\end{array}\right]=\left[\begin{array}{cc}
17 & 1 \\
1 & 5
\end{array}\right] \\
A^{T} \mathbf{b}=\left[\begin{array}{lll}
4 & 0 & 1 \\
0 & 2 & 1
\end{array}\right]\left[\begin{array}{l}
2 \\
0 \\
11
\end{array}\right]=\left[\begin{array}{l}
19 \\
11
\end{array}\right] \\
{\left[\begin{array}{cc}
17 & 1 \\
1 & 5
\end{array}\right] \hat{\mathbf{x}}=\left[\begin{array}{l}
19 \\
11
\end{array}\right]}
\end{gathered}
$$



$$
\left[\begin{array}{ccc}
17 & 1 & 19 \\
1 & 5 & 11
\end{array}\right] \sim\left[\begin{array}{lll}
1 & 0 & 1 \\
0 & 1 & 2
\end{array}\right] \Longrightarrow \hat{\mathbf{x}}=\left[\begin{array}{l}
1 \\
2
\end{array}\right]
$$

</details>

Find the least squares solution to the inconsistent system $A \mathbf{x}=$ b where

$$
A=\left[\begin{array}{llll}
1 & 1 & 0 & 0 \\
1 & 1 & 0 & 0 \\
1 & 0 & 1 & 0 \\
1 & 0 & 1 & 0 \\
1 & 0 & 0 & 1 \\
1 & 0 & 0 & 1
\end{array}\right], \quad \mathbf{b}=\left[\begin{array}{r}
-3 \\
-1 \\
0 \\
2 \\
5 \\
1
\end{array}\right]
$$

<details><summary>Solution</summary>

We solve the normal equations:

$$
A^{T} A=\left[\begin{array}{llllll}
1 & 1 & 1 & 1 & 1 & 1 \\
1 & 1 & 0 & 0 & 0 & 0 \\
0 & 0 & 1 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 1
\end{array}\right]\left[\begin{array}{llll}
1 & 1 & 0 & 0 \\
1 & 1 & 0 & 0 \\
1 & 0 & 1 & 0 \\
1 & 0 & 1 & 0 \\
1 & 0 & 0 & 1 \\
1 & 0 & 0 & 1
\end{array}\right]=\left[\begin{array}{llll}
6 & 2 & 2 & 2 \\
2 & 2 & 0 & 0 \\
2 & 0 & 2 & 0 \\
2 & 0 & 0 & 2
\end{array}\right]
$$

$$
\begin{gathered}
A^{T} \mathbf{b}=\left[\begin{array}{llllll}
1 & 1 & 1 & 1 & 1 & 1 \\
1 & 1 & 0 & 0 & 0 & 0 \\
0 & 0 & 1 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 1
\end{array}\right]\left[\begin{array}{r}
-3 \\
-1 \\
0 \\
2 \\
5 \\
1
\end{array}\right]=\left[\begin{array}{r}
4 \\
-4 \\
2 \\
6
\end{array}\right] \\
A^{T} A \hat{\mathbf{x}}=A^{T} \mathbf{b}
\end{gathered}
$$

$$
\left[\begin{array}{llll}
6 & 2 & 2 & 2 \\
2 & 2 & 0 & 0 \\
2 & 0 & 2 & 0 \\
2 & 0 & 0 & 2
\end{array}\right] \hat{\mathbf{x}}=\left[\begin{array}{r}
4 \\
-4 \\
2 \\
6
\end{array}\right]
$$



$$
\begin{aligned}
& \left[\begin{array}{rrrrr}6 & 2 & 2 & 2 & 4 \\2 & 2 & 0 & 0 & -4 \\2 & 0 & 2 & 0 & 2 \\2 & 0 & 0 & 2 & 6\end{array}\right] \sim\left[\begin{array}{rrrrr}1 & 0 & 0 & 1 & 3 \\0 & 1 & 0 & -1 & -5 \\0 & 0 & 1 & -1 & -2 \\0 & 0 & 0 & 0 & 0\end{array}\right] \\
& x_{1}+x_{4}=3 \quad x_{1}=-x_{4}+3 \\
& x_{2} \quad-x_{4}=-5 \quad \Longrightarrow \quad x_{2}=x_{4}-5 \\
& x_{3}-x_{4}=-2 \quad x_{3}=x_{4}-2 \\
& \hat{\mathbf{x}}=\left[\begin{array}{l}x_{1} \\x_{2} \\x_{3} \\x_{4}\end{array}\right]=\left[\begin{array}{c}-x_{4}+3 \\x_{4}-5 \\x_{4}-2 \\x_{4}\end{array}\right]=x_{4}\left[\begin{array}{r}-1 \\1 \\1 \\1\end{array}\right]+\left[\begin{array}{r}3 \\-5 \\-2 \\0\end{array}\right]
\end{aligned}
$$

</details>
