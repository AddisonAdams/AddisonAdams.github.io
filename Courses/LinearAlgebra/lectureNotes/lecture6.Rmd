---
title: "Matrix Inverses, and Column and Row Spaces"
author: "Brody Erlandson"
output:
  html_document:
    code_folding: hide
    toc: true
    toc_float: true
    toc_depth: 2
---

\include{mathCommands}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Matrix Inverses

## Definitions and Theorems {.tabset}

### Definition 1.1. 
**Invertible Matrix**

An $n \times n$ matrix $\A$ is invertible if there exists an $n \times n$ matrix $\Cmat$ such that $\Cmat\A = \I_n$ and $\A\Cmat = \I_n$. $\Cmat$ is called the inverse of $\A$ and we write $\Cmat = \A^{−1}$.

### Theorem 1.1. 

The inverse of a matrix is unique.

<details><summary>*Proof*</summary> 

Let $\B$ and $\Cmat$ be inverses of $\A$. Then $\B = \B\I = \B\A\Cmat = \I\Cmat = \Cmat$.

</details> \newline 

### Theorem 1.2.
**Computing a Matrix Inverse**

If $\A$ is a nonsingular $n \times n$ matrix and $\bf{D}$ is the matrix formed by augmenting $\A$ with $\I_n$, then the reduced row-echelon form of $\bf{D}$ will be In augmented with
the $n \times n$ matrix $\Cmat$, where $\A\Cmat = \I_n$. We can write this as:

$$\text{If } \mathbf{D} = [\A|\I_n] \sim [\I_n|\Cmat],\text{ then }\A\Cmat = \I_n$$

<details><summary> Explanation </summary> 

We can think of $[\A|\I_n]$ as finding the solution to each of the columns of $\I_n$. Thus if $\A \sim \I_n$, then there is a unique solution to each of the columns of $\I_n$. These solutions are $\Cmat$. By the definition of $\A\Cmat$, we know it is the columns of $\Cmat$ multiplied by $\A$. Which will give us $\I_n$. 

</details> \newline 

### Theorem 1.3.

If $\A$ is an invertible $n \times n$ matrix, then the equation $\A\bf{x}=\bf{b}$
has the unique solution $\mathbf{x} = \A^{−1}\mathbf{b}$ for all $\mathbf{b} \in \C^n$.

<details><summary>*Proof*</summary> 

Let $\A$ be an invertible $n \times n$ matrix, and $\mathbf{x, b} \in \C^n$. We know there is at least one solution because of

$$\A\bf{x}=\bf{b} \Rightarrow \A^{-1}\A\bf{x}=\A^{-1}\bf{b} \Rightarrow \bf{x}=\A^{-1}\bf{b}$$

We know this is unique, because if we assume we have a $\mathbf{y} \in \C^n$ such that $\mathbf{x} \not= \mathbf{y}$, then

$$\A\bf{y}=\bf{b} \Rightarrow \bf{y}=\A^{-1}\bf{b} \Rightarrow \bf{y}=\bf{x}$$

Which is a contradiction.

</details> \newline 

### Theorem 1.4.
**Properties of Inverse Matrices**

1. If $\A$ is invertible, then $\A^{−1}$ is invertible and $(\A^{−1})^{−1} = \A$
2. If $\A$ and $\B$ are $n \times n$ and invertible, then so is $\A\B$, and $(\A\B)^{−1} = \B^{-1}\A^{−1}$
3. If $\A$ is invertible, then so is $\A^T$, and $(\A^T)^{−1} = (\A^{−1})^T = \A^{−T}$
4. If $\A$ is invertible and $\alpha$ is a non-zero scalar, then $\alpha\A$ is invertible, and $(\alpha\A)^{−1} = \frac{1}{\alpha}\A^{−1}$

### Theorem 1.5.

If $\A$ and $\B$ are $n \times n$ matrices, then $\A\B$ is nonsingular iff $\A$ and $\B$ are both nonsingular.

### Theorem 1.6.

If $\A$ and $\B$ are $n \times n$ matrices and $\A\B = \I_n$, then $\B\A = \I_n$.

### Theorem 1.7.
**Equivalence of Nonsingular and Invertible**

If $\A$ is an $n \times n$ matrix, then $\A$ is nonsingular iff $\A$ is invertible

## Examples

Let

$$\A = \mddd{1}{-1}{0}{1}{1}{1}{2}{1}{-1},\ \mathbf b_1 = \vddd{1}{1}{1},\ \mathbf b_2 = \vddd{1}{2}{1},\ \mathbf b_3 = \vddd{1}{2}{3}$$

Find if the system is consistent for $\mathbf b_1,\ \mathbf b_2,\ \mathbf b_3$ given $\A$, and what the solution sets are if consistent.

<details><summary>*Solution*</summary> 

$$\A \sim \begin{bmatrix} 1 & 0 & 0 & .8 & 1 & 1.4 \\
                          0 & 1 & 0 & -.2 & 0 & .4 \\
                          0 & 0 & 1 & .4 & 1 & .2 \\
                          \end{bmatrix}$$
                          
So all are consistent and have one solution.

</details> \newline 

Let 

$$\A = \mddd{1}{-2}{0}{2}{1}{4}{3}{5}{-1}$$

<details><summary>*Solution*</summary> 

$$[\A|\I_3] \sim \begin{bmatrix} 1 & 0 & 0 & \frac{3}{7} & \frac{2}{49} & \frac{8}{49}  \\
                                 0 & 1 & 0 & -\frac{2}{7} & \frac{1}{49} & \frac{4}{49} \\
                                 0 & 0 & 1 & -\frac{1}{7} & \frac{11}{49} & -\frac{5}{49} \\
                                 \end{bmatrix} = [\I_3|\Cmat]$$
                          
So their is an inverse for this matrix, and it is $\Cmat$

</details> \newline

# Invertible Matrix Theorem

## Definitions and Theorems {.tabset}

### Theorem 2.1. 
**Invertible Matrix Theorem (IMT)**

Let $\A$ be an $n \times n$ matrix. The following statements are logically equivalent.

1. $\A$ is an invertible matrix.
2. $\A$ is nonsingular
3. $\A$ is row equivalent to the $n \times n$ identity matrix.
4. $\A$ has $n$ pivot positions.
5. The equation $\A\mathbf{x} = \0$ has only the trivial solution.
6. $\mathcal{N}(\A) = \{\0\}$
7. The columns of $\A$ form a linearly independent set.
8. The linear transformation $\mathbf{x} \mapsto \A\mathbf{x}$ is one-to-one $\mathbf b \in \C^n$.
10. The system $\mathcal{LS}(\A, \mathbf{b})$ has a unique solution for every $\mathbf{b} \in \C^n$.
11. The columns of $\A$ span $\C^n$.
12. The linear transformation $\mathbf{x} \mapsto \A\mathbf{x}$ is onto.
13. There is an $n \times n$ matrix $\Cmat$ such that $\Cmat\A = \I_n$.
14. There is an $n \times n$ matrix $\mathbf{D}$ such that $\A\mathbf{D} = \I_n$.
15. $\A^T$ is an invertible matrix.
16. det $\A \not= 0$.

### Definition 2.1.
**Unitary Matrix**

A unitary matrix $\mathbf{U}$ is an $n \times n$ matrix that satisfies the equation $\mathbf{U}^*\mathbf{U} = \I_n$.

### Theorem 2.2.

If $\mathbf{U}$ is a unitary matrix, then $\mathbf{U}$ is invertible and $\mathbf{U}^{−1} = \mathbf{U}^*$

<details><summary>*Proof*</summary>  

Number 13 of the IMT is satisfied, so we know by definition and uniqueness, we know that $\mathbf{U}^{−1} = \mathbf{U}^*$

</details> \newline 

### Theorem 2.3.

The columns of an $n \times n$ matrix $\mathbf{U}$ form an orthonormal set iff $\mathbf{U}$ is a unitary matrix.

<details><summary>*Proof*</summary> 

Let $S = \{\u_1, \dots, \u_n\}$, where $\u_i \in \C^n$, be the set of columns of matrix $\mathbf{U}$. Now we have the following:

$$S\text{ is orthonormal } 
\\ \iff \innerProd{\u_i}{\u_j} = \begin{cases} 0, \text{if }i \not= j \\ 1, \text{if }i = j \end{cases}
\\ \iff [\mathbf{U^*U}]_{ij}= \begin{cases} 0, \text{if }i \not= j \\ 1, \text{if }i = j \end{cases}
\\ \iff [\mathbf{U^*U}]_{ij}= [\I_n]_{ij}
\\ \iff \mathbf{U^*U} = \I_n
\\ \iff \mathbf{U} \text{ is a unitary matrix}$$

</details> \newline 

### Theorem 2.4.

If $\mathbf{U}$ is an $n \times n$ unitary matrix and $\bf{x}$ and $\bf{y}$ are in $\C^n$, then

$$\innerProd{\bf Ux}{\bf Uy} = \innerProd{\bf x}{\bf y}\text{ and }\norm{\bf Ux} = \norm{\bf x}$$

<details><summary>*Proof*</summary> 

$$\innerProd{\bf Ux}{\bf Uy} = \mathbf{(Ux)^*Uy} 
\\ = \mathbf{x^*U^*Uy}
\\ = \mathbf{x^*\I_ny}
\\ = \innerProd{\bf x}{\bf y}$$

and 

$$\norm{\mathbf{Ux}} = \sqrt{\innerProd{\mathbf{Ux}}{\mathbf{Ux}}} = \norm{\mathbf{x}}$$

</details> \newline 

## Examples

Let

$$\A = \mdd{\frac{1}{\sqrt 2}}{\frac{1}{\sqrt 2}}{\frac{1i}{\sqrt 2}}{\frac{-1i}{\sqrt 2}}$$

Is $\A$ a unitary matrix?

<details><summary>*Solution*</summary> 

We can show this two ways, first:

$$\A^* = \bar{\A}^T = \mdd{\frac{1}{\sqrt 2}}{\frac{-1i}{\sqrt 2}}{\frac{1}{\sqrt 2}}{\frac{1i}{\sqrt 2}} \\
  \A^*\A = \mdd{\frac{1}{\sqrt 2}}{\frac{-1i}{\sqrt 2}}{\frac{1}{\sqrt 2}}{\frac{1i}{\sqrt 2}} \mdd{\frac{1}{\sqrt 2}}{\frac{1}{\sqrt 2}}{\frac{1i}{\sqrt 2}}{\frac{-1i}{\sqrt 2}} = \I_2$$
  
Second:

$$\innerProd{\mathbf a_1}{\mathbf a_2} = \frac{1}{\sqrt 2} * \frac{1}{\sqrt 2} + \overline{\frac{1i}{\sqrt 2}}*\frac{-1i}{\sqrt 2}\\
= \frac{1}{\sqrt 2} * \frac{1}{\sqrt 2} + \frac{-1i}{\sqrt 2}*\frac{-1i}{\sqrt 2} = \frac{1}{2} - \frac{1}{2} = 0$$

and 

$$\norm{\mathbf a_1} = \frac{1}{\sqrt 2} * \frac{1}{\sqrt 2} + \overline{\frac{1i}{\sqrt 2}}\frac{1i}{\sqrt 2} = 1$$

$$\norm{\mathbf a_2} = \frac{1}{\sqrt 2} * \frac{1}{\sqrt 2} + \overline{\frac{-1i}{\sqrt 2}}\frac{-1i}{\sqrt 2} = 1$$

So it is an unitary matrix.

</details> \newline 

# Column Space

## Definitions and Theorems {.tabset}

### Definition 3.1.
**Column Space**

The column space of a matrix $\A$, written $\mathcal{C}(\A)$, is the span of the set of columns of the matrix. If $\A = [\mathbf{a}_1| \mathbf{a}_2|\dots| \mathbf{a}_n]$, where $\mathbf{a}_1, \mathbf{a}_2,\dots, \mathbf{a}_n$ are the columns of $\A$, then $\mathcal{C}(\A) = \span{\{\mathbf{a}_1, \mathbf{a}_2,\dots, \mathbf{a}_n\}}$

### Theorem 3.1.

If A is an $m \times n$ matrix and $\mathbf{b} \in \C^m$, then $\mathbf{b} \in \mathcal{C}(\A)$ iff $\mathcal{LS}(\A, \mathbf{b})$ is consistent.

### Theorem 3.2.

The pivot columns of a matrix $\A$ form a linearly independent set that spans $\mathcal{C}(\A)$.

<details><summary>*Proof*</summary> 

Let $S = \{ \mathbf{a}_1, \dots, \mathbf{a}_n\}$, where $\mathbf{a}_i \in \C^n$, and let $\A = [ \mathbf{a}_1| \dots| \mathbf{a}_n]$. Now let $P = \{ \mathbf{p}_1, \dots, \mathbf{p}_k\}$ and $B = \{ \mathbf{b}_1, \dots, \mathbf{b}_l\}$, where $p + l = n$, $P \subseteq S$, and $B \subseteq S$. Let set $P$ contain all of the pivot columns of $\A$ and set $B$ contain all non-pivot columns of $\A$. $B \subseteq \span{P}$ because every element is linearly dependent on the pivot columns. Also,  $P \subseteq \span{S}$ because $P \subseteq S$. Further, $S \subseteq \span{P}$ because $S = P \cup B$ and $P \subseteq \span{P}$ and $B \subseteq \span{P}$.

</details> \newline 

# Row Space

## Definitions and Theorems {.tabset}

### Definition 4.1.
**Row Space**

The row space of a matrix $\A$, written $\mathcal{R}(\A)$, is the column space of $\A^T$:

$$\mathcal{R}(\A) = \mathcal{C}(\A^T)$$

### Theorem 4.1. 

If $\A \sim \B$, then $\mathcal{R}(\A) = \mathcal{R}(\B)$. If $\B$ is in echelon form, the set of nonzero rows of $\B$ is linearly independent and spans $\mathcal{R}(\B)$ (and $\mathcal{R}(\A)$).

<details><summary>*Proof*</summary> 

The rows of $\B$ are linear combinations of the rows of $\A$ (because they are formed from row operations on the rows of $\A$), so $\mathcal{R}(\B) \subseteq \mathcal{R}(\A)$. Since row operations are reversible, the rows of $\A$ are linear combinations of the rows of $\B$, so $\mathcal{R}(\A) \subseteq \mathcal{R}(\B)$. Therefore $\mathcal{R}(\A) = \mathcal{R}(\B)$.

By the definition of echelon form, no non-zero row of $\B$ is a linear combination of the rows above it, so all non-zero rows of $\B$ form a linearly independent set. In addition, any zero rows at the bottom of $\B$ are linear combinations of the rows above them, by the definition of row operations. Therefore the set of nonzero rows of $\B$ spans $\mathcal R(\B)$. 

</details> \newline 

## Examples

Find linearly independent, spanning sets for $\mathcal R(\A)$, $\mathcal C(\A)$, and $\mathcal N(\A)$, given that $\A \sim \B$.

$$\A = \begin{bmatrix} 1 & 3 & 4 & -1 & 2 \\ 
                       2 & 6 & 6 &  0 & -3 \\
                       3 & 9 & 3 &  6 & -3 \\
                       3 & 9 & 0 &  9 & 0 \\
                       \end{bmatrix},\ \B = \begin{bmatrix} 1 & 3 & 4 & -1 & 2 \\ 
                                                            0 & 0 & 1 & -1 & 1 \\
                                                            0 & 0 & 0 &  0 & -5 \\
                                                            0 & 0 & 0 &  0 & 0 \\
                                                            \end{bmatrix}$$
                                                            

<details><summary>*Solution*</summary> 

For $\mathcal C(\A)$, we can see the first, third, and fifth columns are pivots. So they span $\mathcal C(\A)$. For $\mathcal R(\A)$, the first three rows are non-zero, thus linearly independent. So they span the $\mathcal R(\A)$. Lastly, for $\mathcal N(\A)$, we have to row reduce all the way to get:

$$\A \sim \begin{bmatrix} 1 & 3 & 0 & -3 & 0 \\ 
                          0 & 0 & 1 & -1 & 0 \\
                          0 & 0 & 0 &  0 & 1 \\
                          0 & 0 & 0 &  0 & 0\\
                          \end{bmatrix}$$

So the null space spanning set is:

$$\Bigg\{\begin{bmatrix} -3 \\ 
                         1 \\
                         0 \\
                         0 \\
                         0
                          \end{bmatrix}, \begin{bmatrix} 3 \\ 
                         0 \\
                         1 \\
                         1 \\
                         0
                          \end{bmatrix} \Bigg\}$$

</details> \newline 